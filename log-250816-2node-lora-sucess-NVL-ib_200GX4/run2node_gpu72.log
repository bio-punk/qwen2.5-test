[2025-08-15 16:10:22,923] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-08-15 16:10:26] llamafactory.cli:143 >> Initializing 8 distributed tasks at: 192.168.0.100:29500
[INFO|2025-08-15 16:10:26] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 2, node rank: 1
[2025-08-15 16:10:33,293] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,450] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,565] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,680] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,696] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,747] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,776] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:10:33,784] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-15 16:10:34,825] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:10:34,979] [INFO] [comm.py:669:init_distributed] cdb=None
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-08-15 16:10:35] llamafactory.hparams.parser:410 >> Process rank: 15, world size: 16, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-15 16:10:35,271] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-08-15 16:10:35] llamafactory.hparams.parser:410 >> Process rank: 13, world size: 16, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[2025-08-15 16:10:35,294] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:10:35,302] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:10:35,325] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:10:35,325] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:10:35,418] [INFO] [comm.py:669:init_distributed] cdb=None
[rank15]:[W815 16:10:35.501179976 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 15]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank13]:[W815 16:10:35.585057988 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 13]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 8, world size: 16, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,021 >> loading file chat_template.jinja
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 11, world size: 16, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 14, world size: 16, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 12, world size: 16, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 10, world size: 16, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:10:36] llamafactory.hparams.parser:410 >> Process rank: 9, world size: 16, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2336] 2025-08-15 16:10:36,307 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:750] 2025-08-15 16:10:36,307 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-15 16:10:36,309 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:10:36,309 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2336] 2025-08-15 16:10:36,593 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-08-15 16:10:36] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.
[WARNING|2025-08-15 16:10:36] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-15 16:10:36] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[WARNING|2025-08-15 16:10:36] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-15 16:10:36] llamafactory.data.loader:143 >> Loading dataset identity.json...
[rank14]:[W815 16:10:36.367255440 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 14]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W815 16:10:36.400524634 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank12]:[W815 16:10:36.413668961 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 12]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank10]:[W815 16:10:36.423546155 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank9]:[W815 16:10:36.428364321 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0% 0/91 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 557.78 examples/s]
[INFO|2025-08-15 16:10:36] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...
Converting format of dataset (num_proc=16):   0% 0/999 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100% 999/999 [00:00<00:00, 6096.19 examples/s]
[rank8]:[W815 16:10:37.426348436 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
e2b29d3263f2:9725:9725 [0] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9725:9725 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9725:9725 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9725:9725 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9725:9725 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9725:9725 [0] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9725:9725 [0] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9728:9728 [3] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9728:9728 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9728:9728 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9728:9728 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9728:9728 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9728:9728 [3] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9728:9728 [3] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9729:9729 [4] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9729:9729 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9729:9729 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9729:9729 [4] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9729:9729 [4] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9729:9729 [4] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9729:9729 [4] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9731:9731 [6] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9731:9731 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9731:9731 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9731:9731 [6] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9731:9731 [6] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9731:9731 [6] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9731:9731 [6] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9727:9727 [2] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9727:9727 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9727:9727 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9727:9727 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9727:9727 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9727:9727 [2] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9726:9726 [1] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9726:9726 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9726:9726 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9726:9726 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9726:9726 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9726:9726 [1] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9730:9730 [5] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9730:9730 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9730:9730 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9727:9727 [2] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9730:9730 [5] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9730:9730 [5] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9730:9730 [5] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9732:9732 [7] NCCL INFO cudaDriverVersion 12040
e2b29d3263f2:9732:9732 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9732:9732 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.101<0>
e2b29d3263f2:9726:9726 [1] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9730:9730 [5] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9732:9732 [7] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
e2b29d3263f2:9732:9732 [7] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
e2b29d3263f2:9732:9732 [7] NCCL INFO NET/Plugin: Using internal network plugin.
e2b29d3263f2:9732:9732 [7] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9725:11470 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9725:11470 [0] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9725:11470 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9725:11470 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9725:11470 [0] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9725:11470 [0] NCCL INFO Using network Socket
e2b29d3263f2:9728:11471 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9728:11471 [3] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9728:11471 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9728:11471 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9728:11471 [3] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9728:11471 [3] NCCL INFO Using network Socket
e2b29d3263f2:9729:11472 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9729:11472 [4] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9729:11472 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9729:11472 [4] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9729:11472 [4] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9729:11472 [4] NCCL INFO Using network Socket
e2b29d3263f2:9731:11473 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9731:11473 [6] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9731:11473 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9731:11473 [6] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9731:11473 [6] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9731:11473 [6] NCCL INFO Using network Socket
e2b29d3263f2:9726:11475 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9726:11475 [1] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9726:11475 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9726:11475 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9726:11475 [1] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9726:11475 [1] NCCL INFO Using network Socket
e2b29d3263f2:9730:11476 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9730:11476 [5] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9730:11476 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9730:11476 [5] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9730:11476 [5] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9730:11476 [5] NCCL INFO Using network Socket
e2b29d3263f2:9727:11474 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9727:11474 [2] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9727:11474 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9727:11474 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9727:11474 [2] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9727:11474 [2] NCCL INFO Using network Socket
e2b29d3263f2:9732:11477 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
e2b29d3263f2:9732:11477 [7] NCCL INFO Failed to open libibverbs.so[.1]
e2b29d3263f2:9732:11477 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
e2b29d3263f2:9732:11477 [7] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.101<0>
e2b29d3263f2:9732:11477 [7] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9732:11477 [7] NCCL INFO Using network Socket
e2b29d3263f2:9730:11476 [5] NCCL INFO ncclCommInitRank comm 0x55a093a27fe0 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9731:11473 [6] NCCL INFO ncclCommInitRank comm 0x55ee16369d30 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9732:11477 [7] NCCL INFO ncclCommInitRank comm 0x56143182ef60 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9729:11472 [4] NCCL INFO ncclCommInitRank comm 0x55b0187aa400 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9728:11471 [3] NCCL INFO ncclCommInitRank comm 0x55f8bcaa8150 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9727:11474 [2] NCCL INFO ncclCommInitRank comm 0x56340e90b430 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9726:11475 [1] NCCL INFO ncclCommInitRank comm 0x563d5b061bb0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9725:11470 [0] NCCL INFO ncclCommInitRank comm 0x55d05efb9340 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0x56347988b920d15b - Init START
e2b29d3263f2:9730:11476 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9730:11476 [5] NCCL INFO NVLS multicast support is not available on dev 5
e2b29d3263f2:9726:11475 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
e2b29d3263f2:9726:11475 [1] NCCL INFO NVLS multicast support is not available on dev 1
e2b29d3263f2:9728:11471 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
e2b29d3263f2:9728:11471 [3] NCCL INFO NVLS multicast support is not available on dev 3
e2b29d3263f2:9732:11477 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9732:11477 [7] NCCL INFO NVLS multicast support is not available on dev 7
e2b29d3263f2:9725:11470 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
e2b29d3263f2:9725:11470 [0] NCCL INFO NVLS multicast support is not available on dev 0
e2b29d3263f2:9727:11474 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
e2b29d3263f2:9727:11474 [2] NCCL INFO NVLS multicast support is not available on dev 2
e2b29d3263f2:9729:11472 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9729:11472 [4] NCCL INFO NVLS multicast support is not available on dev 4
e2b29d3263f2:9731:11473 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9731:11473 [6] NCCL INFO NVLS multicast support is not available on dev 6
e2b29d3263f2:9732:11477 [7] NCCL INFO comm 0x56143182ef60 rank 15 nRanks 16 nNodes 2 localRanks 8 localRank 7 MNNVL 0
e2b29d3263f2:9730:11476 [5] NCCL INFO comm 0x55a093a27fe0 rank 13 nRanks 16 nNodes 2 localRanks 8 localRank 5 MNNVL 0
e2b29d3263f2:9732:11477 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14
e2b29d3263f2:9729:11472 [4] NCCL INFO comm 0x55b0187aa400 rank 12 nRanks 16 nNodes 2 localRanks 8 localRank 4 MNNVL 0
e2b29d3263f2:9732:11477 [7] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9730:11476 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12
e2b29d3263f2:9725:11470 [0] NCCL INFO comm 0x55d05efb9340 rank 8 nRanks 16 nNodes 2 localRanks 8 localRank 0 MNNVL 0
e2b29d3263f2:9730:11476 [5] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9729:11472 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11
e2b29d3263f2:9729:11472 [4] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9726:11475 [1] NCCL INFO comm 0x563d5b061bb0 rank 9 nRanks 16 nNodes 2 localRanks 8 localRank 1 MNNVL 0
e2b29d3263f2:9725:11470 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/0/-1->8->-1
e2b29d3263f2:9725:11470 [0] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9727:11474 [2] NCCL INFO comm 0x56340e90b430 rank 10 nRanks 16 nNodes 2 localRanks 8 localRank 2 MNNVL 0
e2b29d3263f2:9731:11473 [6] NCCL INFO comm 0x55ee16369d30 rank 14 nRanks 16 nNodes 2 localRanks 8 localRank 6 MNNVL 0
e2b29d3263f2:9726:11475 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8
e2b29d3263f2:9726:11475 [1] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9728:11471 [3] NCCL INFO comm 0x55f8bcaa8150 rank 11 nRanks 16 nNodes 2 localRanks 8 localRank 3 MNNVL 0
e2b29d3263f2:9727:11474 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
e2b29d3263f2:9727:11474 [2] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9731:11473 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13
e2b29d3263f2:9731:11473 [6] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9728:11471 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10
e2b29d3263f2:9728:11471 [3] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9730:11476 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9729:11472 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9727:11474 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9730:11476 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9731:11473 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9729:11472 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9727:11474 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9728:11471 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9731:11473 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9728:11471 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 01/0 : 1[1] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 01/0 : 9[1] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9732:11477 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9732:11477 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9729:11472 [4] NCCL INFO Connected all rings
e2b29d3263f2:9726:11475 [1] NCCL INFO Connected all rings
e2b29d3263f2:9728:11471 [3] NCCL INFO Connected all rings
e2b29d3263f2:9727:11474 [2] NCCL INFO Connected all rings
e2b29d3263f2:9730:11476 [5] NCCL INFO Connected all rings
e2b29d3263f2:9725:11470 [0] NCCL INFO Connected all rings
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9731:11473 [6] NCCL INFO Connected all rings
e2b29d3263f2:9732:11477 [7] NCCL INFO Connected all rings
e2b29d3263f2:9729:11472 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9727:11474 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9728:11471 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9730:11476 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9729:11472 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9727:11474 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9731:11473 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9728:11471 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9730:11476 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9731:11473 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM/read
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9725:11470 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9726:11475 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM/read
e2b29d3263f2:9729:11472 [4] NCCL INFO Connected all trees
e2b29d3263f2:9729:11472 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9729:11472 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9728:11471 [3] NCCL INFO Connected all trees
e2b29d3263f2:9728:11471 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9728:11471 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9732:11477 [7] NCCL INFO Connected all trees
e2b29d3263f2:9732:11477 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9732:11477 [7] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9730:11476 [5] NCCL INFO Connected all trees
e2b29d3263f2:9731:11473 [6] NCCL INFO Connected all trees
e2b29d3263f2:9730:11476 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9730:11476 [5] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9731:11473 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9731:11473 [6] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9727:11474 [2] NCCL INFO Connected all trees
e2b29d3263f2:9727:11474 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9727:11474 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9725:11470 [0] NCCL INFO Connected all trees
e2b29d3263f2:9726:11475 [1] NCCL INFO Connected all trees
e2b29d3263f2:9725:11470 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9725:11470 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9726:11475 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9726:11475 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9725:11470 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9725:11470 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9725:11470 [0] NCCL INFO ncclCommInitRank comm 0x55d05efb9340 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9732:11477 [7] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9729:11472 [4] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9732:11477 [7] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9727:11474 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9730:11476 [5] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9729:11472 [4] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9732:11477 [7] NCCL INFO ncclCommInitRank comm 0x56143182ef60 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9727:11474 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9730:11476 [5] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9729:11472 [4] NCCL INFO ncclCommInitRank comm 0x55b0187aa400 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9727:11474 [2] NCCL INFO ncclCommInitRank comm 0x56340e90b430 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9730:11476 [5] NCCL INFO ncclCommInitRank comm 0x55a093a27fe0 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9726:11475 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9726:11475 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9726:11475 [1] NCCL INFO ncclCommInitRank comm 0x563d5b061bb0 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9728:11471 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9728:11471 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9728:11471 [3] NCCL INFO ncclCommInitRank comm 0x55f8bcaa8150 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0x56347988b920d15b - Init COMPLETE
e2b29d3263f2:9731:11473 [6] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
e2b29d3263f2:9731:11473 [6] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
e2b29d3263f2:9731:11473 [6] NCCL INFO ncclCommInitRank comm 0x55ee16369d30 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0x56347988b920d15b - Init COMPLETE
Running tokenizer on dataset (num_proc=16):   0% 0/1090 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6% 69/1090 [00:00<00:07, 133.90 examples/s]Running tokenizer on dataset (num_proc=16):  25% 274/1090 [00:00<00:01, 458.41 examples/s]Running tokenizer on dataset (num_proc=16):  38% 410/1090 [00:00<00:01, 537.72 examples/s]Running tokenizer on dataset (num_proc=16):  50% 546/1090 [00:01<00:00, 601.29 examples/s]Running tokenizer on dataset (num_proc=16):  63% 682/1090 [00:01<00:00, 552.75 examples/s]Running tokenizer on dataset (num_proc=16):  81% 886/1090 [00:01<00:00, 697.36 examples/s]Running tokenizer on dataset (num_proc=16):  94% 1022/1090 [00:01<00:00, 720.78 examples/s]Running tokenizer on dataset (num_proc=16): 100% 1090/1090 [00:01<00:00, 582.11 examples/s]
training example:
input_ids:
[103929, 98380, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 100772, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 104139, 100661, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 104139, 112526, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 100160, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 100678, 47606, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 101398, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 104559, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 99257, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 104116, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 100007, 53481, 99283, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 106961, 110498, 109916, 99604, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 18830, 113065, 101037, 11319, 27, 91, 408, 3575, 4326, 91, 29, 102762, 100153, 107494, 107120, 101037, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 100007, 54542, 20002, 105918, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 100006, 99553, 102224, 109963, 100364, 11319, 27, 91, 408, 3575, 4326, 91, 29, 56568, 32664, 20002, 101080, 103936, 104139, 104108, 11319, 27, 91, 408, 3575, 4326, 91, 29, 103929, 102104, 64471, 73670, 109874, 11319, 27, 91, 408, 3575, 4326, 91, 29, 105043, 5002, 15469, 100013, 9370, 99245, 11319, 27, 91, 408, 3575, 4326, 91, 29, 100622, 15672, 38, 2828, 3837, 103929, 98380, 102021, 11319, 27, 91, 408, 3575, 4326, 91, 29, 5002, 15469, 107781, 103963, 56568, 11319, 27, 91, 408, 3575, 4326, 91, 29, 105043, 5002, 15469, 100013, 9370, 15672, 38, 2828, 101037, 11319, 27, 91, 408, 3575, 4326, 91, 29, 74785, 264, 1882, 315, 3259, 1884, 20352, 15757, 91, 408, 3575, 4326, 91, 29, 8963, 279, 2701, 11652, 1667, 264, 73350, 25, 576, 1803, 85610, 6157, 15757, 91, 408, 3575, 4326, 91, 29, 8078, 264, 65243, 5693, 311, 11926, 33878, 15757, 91, 408, 3575, 4326, 91, 29, 641, 684, 264, 501, 3409, 553, 34171, 1378, 6350, 4244, 15757, 91, 408, 3575, 4326, 91, 29, 35127, 458, 3110, 315, 264, 2618, 429, 264, 6366, 646, 653, 2664, 1091, 264, 3738, 1660, 15757, 91, 408, 3575, 4326, 91, 29, 22043, 279, 5029, 315, 264, 21495, 11, 1477, 700, 1181, 46342, 624, 16384, 220, 16, 284, 220, 19, 198, 16384, 220, 17, 284, 220, 21, 198, 16384, 220, 18, 284, 220, 23, 27, 91, 408, 3575, 4326, 91, 29, 4021, 458, 7373, 220, 16, 19, 15, 3668, 22272, 1736, 27, 91, 408, 3575, 4326, 91, 29, 1336, 13373, 264, 1140, 315, 279, 1909, 220, 20, 23677, 4217, 304, 220, 17, 15, 17, 16, 15757, 91, 408, 3575, 4326, 91, 29, 58465, 539, 419, 11652, 311, 5263, 31273, 198, 785, 4522, 315, 1059, 1660, 773, 33200, 1865, 752, 15289, 27, 91, 408, 3575, 4326, 91, 29, 840, 20772, 279, 11799, 1948, 19654, 323, 55569, 27, 91, 408, 3575, 4326, 91, 29, 31115, 264, 825, 1331, 18380, 2265, 369, 264, 11521, 11116, 15757, 91, 408, 3575, 4326, 91, 29, 840, 20772, 279, 7286, 315, 384, 41585, 15757, 91, 408, 3575, 4326, 91, 29, 20470, 458, 9342, 311, 15442, 279, 40165, 315, 279, 10981, 1714, 624, 2008, 19586, 6730, 25, 60477, 40956, 27, 91, 408, 3575, 4326, 91, 29, 31115, 264, 1140, 315, 4236, 2155, 6467, 911, 8038, 15757, 91, 408, 3575, 4326, 91, 29, 65077, 26413, 1045, 7488, 429, 1410, 1281, 458, 304, 28045, 975, 6438, 803, 22570, 15757, 91, 408, 3575, 4326, 91, 29, 65077, 26413, 264, 1140, 315, 15311, 369, 264, 6548, 8017, 27, 91, 408, 3575, 4326, 91, 29, 58465, 1247, 279, 11652, 773, 429, 432, 594, 304, 279, 3042, 42687, 624, 7941, 1030, 6439, 518, 279, 2813, 369, 279, 3267, 220, 18, 1635, 15757, 91, 408, 3575, 4326, 91, 29, 2589, 2689, 279, 3897, 21646, 311, 1281, 432, 803, 69846, 624, 10234, 1521, 279, 59881, 5312, 279, 5636, 75414, 91, 408, 3575, 4326, 91, 29, 4021, 458, 15235, 6236, 6331, 27, 91, 408, 3575, 4326, 91, 29, 840, 20772, 1128, 264, 16224, 66767, 374, 15757, 91, 408, 3575, 4326, 91, 29, 4021, 264, 3364, 15860, 264, 7404, 8644, 323, 458, 45740, 15757, 91, 408, 3575, 4326, 91, 29, 53544, 279, 1790, 17795, 5185, 2661, 279, 17795, 8500, 624, 35, 468, 479, 425, 356, 27, 91, 408, 3575, 4326, 91, 29, 3830, 279, 1140, 315, 4244, 11, 10542, 279, 1378, 37328, 23628, 3196, 7831, 315, 279, 3409, 364, 258, 38768, 23569, 641, 38768, 27, 91, 408, 3575, 4326, 91, 29, 675, 2326, 90871, 429, 12598, 1265, 633, 27, 91, 408, 3575, 4326, 91, 29, 35127, 752, 1378, 10295, 315, 32168, 4802, 8173, 15757, 91, 408, 3575, 4326, 91, 29, 10231, 279, 6467, 1119, 1378, 5203, 11, 16989, 323, 2477, 73431, 624, 61686, 594, 50579, 304, 88924, 11, 576, 17358, 304, 279, 21341, 11, 13630, 4492, 596, 11, 3017, 4509, 49328, 27, 91, 408, 3575, 4326, 91, 29, 3838, 525, 279, 7567, 315, 50482, 75414, 91, 408, 3575, 4326, 91, 29, 4340, 1035, 498, 6923, 4194, 5109, 1948, 220, 16, 323, 220, 16, 15, 304, 7943, 75414, 91, 408, 3575, 4326, 91, 29, 56808, 279, 2701, 3491, 25, 220, 24, 481, 220, 17, 856, 220, 18, 27, 91, 408, 3575, 4326, 91, 29, 14449, 304, 279, 10113, 1667, 264, 3409, 429, 1850, 44595, 279, 11652, 624, 785, 3283, 572, 10113, 24481, 304, 264, 12045, 6193, 315, 96283, 30743, 15757, 91, 408, 3575, 4326, 91, 29, 74785, 1246, 5662, 6832, 374, 1483, 304, 419, 1849, 624, 32, 1849, 429, 44699, 1424, 66283, 18509, 15757, 91, 408, 3575, 4326, 91, 29, 31115, 264, 7327, 911, 18707, 15757, 91, 408, 3575, 4326, 91, 29, 675, 825, 315, 279, 23091, 315, 8038, 27, 91, 408, 3575, 4326, 91, 29, 7985, 279, 2487, 315, 458, 2551, 311, 21399, 1251, 311, 264, 62560, 389, 279, 2661, 8544, 624, 26406, 25, 2585, 311, 990, 821, 27875, 311, 7269, 697, 2562, 15757, 91, 408, 3575, 4326, 91, 29, 2212, 220, 18, 10295, 311, 279, 2701, 11652, 624, 63907, 9170, 304, 279, 3639, 4180, 646, 1102, 304, 1112, 27, 91, 408, 3575, 4326, 91, 29, 82345, 279, 2701, 3717, 438, 830, 11, 895, 11, 476, 35118, 13, 9258, 220, 16, 369]
inputs:
你的功能是什么？<|end_of_text|>你的特点是什么？<|end_of_text|>你有什么优势？<|end_of_text|>你有什么特长？<|end_of_text|>你的目标是什么？<|end_of_text|>你为什么存在？<|end_of_text|>你的使命是什么？<|end_of_text|>你的职责是什么？<|end_of_text|>你的工作是什么？<|end_of_text|>你的定位是什么？<|end_of_text|>你如何描述自己？<|end_of_text|>你与其他助手有何不同？<|end_of_text|>你有创造力吗？<|end_of_text|>你会保护用户的隐私吗？<|end_of_text|>你如何处理用户的数据？<|end_of_text|>你能够提供哪些类型的帮助？<|end_of_text|>你对用户提出的问题有什么限制？<|end_of_text|>你的回答是否可以信赖？<|end_of_text|>你是OpenAI开发的什么？<|end_of_text|>作为ChatGPT，你的功能是什么？<|end_of_text|>OpenAI为什么要制作你？<|end_of_text|>你是OpenAI开发的ChatGPT吗？<|end_of_text|>Describe a process of making crepes.<|end_of_text|>Transform the following sentence using a synonym: The car sped quickly.<|end_of_text|>Make a persuasive argument to promote recycling.<|end_of_text|>Invent a new word by combining two existing words.<|end_of_text|>Give an example of a job that a computer can do better than a human being.<|end_of_text|>Given the parameters of a triangle, find out its perimeter.
Side 1 = 4
Side 2 = 6
Side 3 = 8<|end_of_text|>Create an effective 140 character twitter post<|end_of_text|>Produce a list of the top 5 NHL players in 2021.<|end_of_text|>Reword this sentence to increase clarity
The idea of her being so brave made me smile<|end_of_text|>Explain the differences between birds and mammals<|end_of_text|>Generate a one-sentence title for a creative recipe.<|end_of_text|>Explain the concept of e-commerce.<|end_of_text|>Design an experiment to evaluate the efficacy of the proposed method.
Proposed Method: Neural persistence<|end_of_text|>Generate a list of five different books about science.<|end_of_text|>Brainstorm some activities that could make an in-person work meeting more engaging.<|end_of_text|>Brainstorm a list of titles for a photo album<|end_of_text|>Rewrite the sentence so that it's in the present tense.
She had worked at the company for the past 3 years.<|end_of_text|>Adapt the provided joke to make it more humorous.
Why did the frog cross the road?<|end_of_text|>Create an AI chatbot<|end_of_text|>Explain what a circuit breaker is.<|end_of_text|>Create a story involving a talking mouse and an elephant.<|end_of_text|>Predict the next musical note given the musical sequence.
D E G B C<|end_of_text|>From the list of words, identify the two-word compound antonym of the word 'injustice'.
Injustice<|end_of_text|>Name three vaccinations that adults should get<|end_of_text|>Give me two examples of renewable energy sources.<|end_of_text|>Sort the books into two groups, fiction and non-fiction.
Alice's Adventures in Wonderland, The Cat in the Hat, Wild Swans, My Struggle<|end_of_text|>What are the benefits of exercising?<|end_of_text|>How would you generate random numbers between 1 and 10 in Java?<|end_of_text|>Resolve the following problem: 9 - 2 x 3<|end_of_text|>Fill in the blank using a word that best completes the sentence.
The city was blanketed in a thick layer of eerie ____.<|end_of_text|>Describe how machine learning is used in this system.
A system that recognizes hand-written digits.<|end_of_text|>Generate a rap about dreams.<|end_of_text|>Name one of the branches of science<|end_of_text|>Write the body of an email to invite people to a webinar on the given topic.
Topic: How to use data analytics to improve your business.<|end_of_text|>Add 3 examples to the following sentence.
Gun violence in the United States can result in...<|end_of_text|>Evaluate the following claim as true, false, or uncertain. Output 1 for
[INFO|configuration_utils.py:750] 2025-08-15 16:10:43,236 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-15 16:10:43,237 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-08-15 16:10:43] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1305] 2025-08-15 16:10:43,359 >> loading weights file /data/Qwen2.5-32B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:4363] 2025-08-15 16:10:43,359 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-08-15 16:10:43,359] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[INFO|configuration_utils.py:1098] 2025-08-15 16:10:43,368 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[2025-08-15 16:10:43,476] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,477] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,479] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,479] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,483] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,488] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:10:43,490] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6% 1/17 [00:08<02:12,  8.26s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:12,  8.26s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:11,  8.25s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:12,  8.26s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:11,  8.25s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:11,  8.25s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:12,  8.25s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:11,  8.25s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:57,  7.86s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:48,  7.76s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.72s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:32,  7.69s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:24,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:16,  7.67s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:08,  7.66s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:01,  7.65s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:53,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.63s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  71% 12/17 [01:32<00:38,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:30,  7.61s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  82% 14/17 [01:47<00:22,  7.60s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.61s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards:  94% 16/17 [02:02<00:07,  7.62s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]
Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]
Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]
Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]

Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]
Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]
[INFO|modeling_utils.py:5606] 2025-08-15 16:13:46,398 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:5614] 2025-08-15 16:13:46,398 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data/Qwen2.5-32B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
Loading checkpoint shards: 100% 17/17 [02:04<00:00,  5.90s/it]Loading checkpoint shards: 100% 17/17 [02:04<00:00,  7.32s/it]
[INFO|configuration_utils.py:1051] 2025-08-15 16:13:46,402 >> loading configuration file /data/Qwen2.5-32B-Instruct/generation_config.json
[INFO|configuration_utils.py:1098] 2025-08-15 16:13:46,403 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-08-15 16:13:46] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-15 16:13:46] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-15 16:13:46] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-08-15 16:13:46] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-08-15 16:13:46] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,q_proj,k_proj,up_proj,v_proj,gate_proj,o_proj
[INFO|2025-08-15 16:14:04] llamafactory.model.loader:143 >> trainable params: 67,108,864 || all params: 32,830,985,216 || trainable%: 0.2044
[INFO|trainer.py:757] 2025-08-15 16:14:04,944 >> Using auto half precision backend
[2025-08-15 16:14:05,421] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[INFO|trainer.py:2433] 2025-08-15 16:14:28,840 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-15 16:14:28,840 >>   Num examples = 15
[INFO|trainer.py:2435] 2025-08-15 16:14:28,840 >>   Num Epochs = 3
[INFO|trainer.py:2436] 2025-08-15 16:14:28,840 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2439] 2025-08-15 16:14:28,840 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2440] 2025-08-15 16:14:28,840 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-15 16:14:28,840 >>   Total optimization steps = 3
[INFO|trainer.py:2442] 2025-08-15 16:14:28,848 >>   Number of trainable parameters = 67,108,864
e2b29d3263f2:9726:11988 [1] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9727:12005 [2] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9729:12015 [4] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9728:11998 [3] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9730:11984 [5] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9731:12033 [6] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9732:12026 [7] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9725:12035 [0] NCCL INFO Comm config Blocking set to 1
e2b29d3263f2:9729:12045 [4] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9729:12045 [4] NCCL INFO Using network Socket
e2b29d3263f2:9730:12046 [5] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9726:12043 [1] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9727:12044 [2] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9730:12046 [5] NCCL INFO Using network Socket
e2b29d3263f2:9728:12047 [3] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9726:12043 [1] NCCL INFO Using network Socket
e2b29d3263f2:9727:12044 [2] NCCL INFO Using network Socket
e2b29d3263f2:9728:12047 [3] NCCL INFO Using network Socket
e2b29d3263f2:9732:12049 [7] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9732:12049 [7] NCCL INFO Using network Socket
e2b29d3263f2:9731:12048 [6] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9731:12048 [6] NCCL INFO Using network Socket
e2b29d3263f2:9725:12050 [0] NCCL INFO Using non-device net plugin version 0
e2b29d3263f2:9725:12050 [0] NCCL INFO Using network Socket
e2b29d3263f2:9730:12046 [5] NCCL INFO ncclCommInitRank comm 0x7f2ddcedb110 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9729:12045 [4] NCCL INFO ncclCommInitRank comm 0x7f090cedd090 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9731:12048 [6] NCCL INFO ncclCommInitRank comm 0x7f4f8cedc210 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9728:12047 [3] NCCL INFO ncclCommInitRank comm 0x7f700cedc6f0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9725:12050 [0] NCCL INFO ncclCommInitRank comm 0x7fbcecedb640 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9727:12044 [2] NCCL INFO ncclCommInitRank comm 0x7f10d0edc370 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9726:12043 [1] NCCL INFO ncclCommInitRank comm 0x7f327cede180 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9732:12049 [7] NCCL INFO ncclCommInitRank comm 0x7f6d1cedc3f0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xba6f4dab8fc0996d - Init START
e2b29d3263f2:9725:12050 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
e2b29d3263f2:9725:12050 [0] NCCL INFO NVLS multicast support is not available on dev 0
e2b29d3263f2:9731:12048 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9731:12048 [6] NCCL INFO NVLS multicast support is not available on dev 6
e2b29d3263f2:9728:12047 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,00000000,ffffffff
e2b29d3263f2:9728:12047 [3] NCCL INFO NVLS multicast support is not available on dev 3
e2b29d3263f2:9730:12046 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9730:12046 [5] NCCL INFO NVLS multicast support is not available on dev 5
e2b29d3263f2:9732:12049 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9732:12049 [7] NCCL INFO NVLS multicast support is not available on dev 7
e2b29d3263f2:9729:12045 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
e2b29d3263f2:9729:12045 [4] NCCL INFO NVLS multicast support is not available on dev 4
e2b29d3263f2:9727:12044 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff
e2b29d3263f2:9727:12044 [2] NCCL INFO NVLS multicast support is not available on dev 2
e2b29d3263f2:9726:12043 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff
e2b29d3263f2:9726:12043 [1] NCCL INFO NVLS multicast support is not available on dev 1
e2b29d3263f2:9725:12050 [0] NCCL INFO comm 0x7fbcecedb640 rank 8 nRanks 16 nNodes 2 localRanks 8 localRank 0 MNNVL 0
e2b29d3263f2:9732:12049 [7] NCCL INFO comm 0x7f6d1cedc3f0 rank 15 nRanks 16 nNodes 2 localRanks 8 localRank 7 MNNVL 0
e2b29d3263f2:9725:12050 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/0/-1->8->-1
e2b29d3263f2:9730:12046 [5] NCCL INFO comm 0x7f2ddcedb110 rank 13 nRanks 16 nNodes 2 localRanks 8 localRank 5 MNNVL 0
e2b29d3263f2:9725:12050 [0] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9731:12048 [6] NCCL INFO comm 0x7f4f8cedc210 rank 14 nRanks 16 nNodes 2 localRanks 8 localRank 6 MNNVL 0
e2b29d3263f2:9726:12043 [1] NCCL INFO comm 0x7f327cede180 rank 9 nRanks 16 nNodes 2 localRanks 8 localRank 1 MNNVL 0
e2b29d3263f2:9729:12045 [4] NCCL INFO comm 0x7f090cedd090 rank 12 nRanks 16 nNodes 2 localRanks 8 localRank 4 MNNVL 0
e2b29d3263f2:9728:12047 [3] NCCL INFO comm 0x7f700cedc6f0 rank 11 nRanks 16 nNodes 2 localRanks 8 localRank 3 MNNVL 0
e2b29d3263f2:9727:12044 [2] NCCL INFO comm 0x7f10d0edc370 rank 10 nRanks 16 nNodes 2 localRanks 8 localRank 2 MNNVL 0
e2b29d3263f2:9730:12046 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12
e2b29d3263f2:9732:12049 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14
e2b29d3263f2:9730:12046 [5] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9729:12045 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11
e2b29d3263f2:9732:12049 [7] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9729:12045 [4] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9731:12048 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13
e2b29d3263f2:9726:12043 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8
e2b29d3263f2:9731:12048 [6] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9727:12044 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
e2b29d3263f2:9728:12047 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10
e2b29d3263f2:9726:12043 [1] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9727:12044 [2] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9728:12047 [3] NCCL INFO P2P Chunksize set to 131072
e2b29d3263f2:9729:12045 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9730:12046 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9731:12048 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9729:12045 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9728:12047 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9730:12046 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9727:12044 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9731:12048 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9728:12047 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9727:12044 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 01/0 : 1[1] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 00/0 : 8[0] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 01/0 : 9[1] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9732:12049 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9732:12049 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9729:12045 [4] NCCL INFO Connected all rings
e2b29d3263f2:9729:12045 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9729:12045 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/CUMEM/read
e2b29d3263f2:9726:12043 [1] NCCL INFO Connected all rings
e2b29d3263f2:9728:12047 [3] NCCL INFO Connected all rings
e2b29d3263f2:9727:12044 [2] NCCL INFO Connected all rings
e2b29d3263f2:9725:12050 [0] NCCL INFO Connected all rings
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9728:12047 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9727:12044 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM/read
e2b29d3263f2:9730:12046 [5] NCCL INFO Connected all rings
e2b29d3263f2:9732:12049 [7] NCCL INFO Connected all rings
e2b29d3263f2:9731:12048 [6] NCCL INFO Connected all rings
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM/read
e2b29d3263f2:9728:12047 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/CUMEM/read
e2b29d3263f2:9727:12044 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM/read
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/Socket/0
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM/read
e2b29d3263f2:9730:12046 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9731:12048 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9725:12050 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/Socket/0
e2b29d3263f2:9730:12046 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/CUMEM/read
e2b29d3263f2:9726:12043 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM/read
e2b29d3263f2:9731:12048 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/CUMEM/read
e2b29d3263f2:9732:12049 [7] NCCL INFO Connected all trees
e2b29d3263f2:9732:12049 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9732:12049 [7] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9728:12047 [3] NCCL INFO Connected all trees
e2b29d3263f2:9728:12047 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9728:12047 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9731:12048 [6] NCCL INFO Connected all trees
e2b29d3263f2:9731:12048 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9731:12048 [6] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9729:12045 [4] NCCL INFO Connected all trees
e2b29d3263f2:9730:12046 [5] NCCL INFO Connected all trees
e2b29d3263f2:9729:12045 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9730:12046 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9729:12045 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9730:12046 [5] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9727:12044 [2] NCCL INFO Connected all trees
e2b29d3263f2:9727:12044 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9727:12044 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9725:12050 [0] NCCL INFO Connected all trees
e2b29d3263f2:9726:12043 [1] NCCL INFO Connected all trees
e2b29d3263f2:9725:12050 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9725:12050 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9726:12043 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
e2b29d3263f2:9726:12043 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
e2b29d3263f2:9731:12048 [6] NCCL INFO ncclCommInitRank comm 0x7f4f8cedc210 rank 14 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9725:12050 [0] NCCL INFO ncclCommInitRank comm 0x7fbcecedb640 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9729:12045 [4] NCCL INFO ncclCommInitRank comm 0x7f090cedd090 rank 12 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9730:12046 [5] NCCL INFO ncclCommInitRank comm 0x7f2ddcedb110 rank 13 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9732:12049 [7] NCCL INFO ncclCommInitRank comm 0x7f6d1cedc3f0 rank 15 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9728:12047 [3] NCCL INFO ncclCommInitRank comm 0x7f700cedc6f0 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9726:12043 [1] NCCL INFO ncclCommInitRank comm 0x7f327cede180 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xba6f4dab8fc0996d - Init COMPLETE
e2b29d3263f2:9727:12044 [2] NCCL INFO ncclCommInitRank comm 0x7f10d0edc370 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xba6f4dab8fc0996d - Init COMPLETE
^[[A    [2025-08-15 16:23:24,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-3/global_step0/zero_pp_rank_8_mp_rank_00_model_states.pt...
[2025-08-15 16:23:24,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-3/global_step0/zero_pp_rank_8_mp_rank_00_model_states.pt.
[2025-08-15 16:23:24,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-3/global_step0/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt...
[2025-08-15 16:23:24,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-3/global_step0/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt.
[2025-08-15 16:23:24,384] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-3/global_step0/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt
[2025-08-15 16:23:24,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step0 is ready now!
[INFO|trainer.py:2718] 2025-08-15 16:23:24,751 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


e2b29d3263f2:9725:9725 [0] NCCL INFO comm 0x7fbcecedb640 rank 8 nranks 16 cudaDev 0 busId e000 - Destroy COMPLETE
e2b29d3263f2:9725:9725 [0] NCCL INFO comm 0x55d05efb9340 rank 8 nranks 16 cudaDev 0 busId e000 - Destroy COMPLETE
e2b29d3263f2:9727:9727 [2] NCCL INFO comm 0x7f10d0edc370 rank 10 nranks 16 cudaDev 2 busId 1f000 - Destroy COMPLETE
e2b29d3263f2:9732:9732 [7] NCCL INFO comm 0x7f6d1cedc3f0 rank 15 nranks 16 cudaDev 7 busId cf000 - Destroy COMPLETE
e2b29d3263f2:9731:9731 [6] NCCL INFO comm 0x7f4f8cedc210 rank 14 nranks 16 cudaDev 6 busId ce000 - Destroy COMPLETE
e2b29d3263f2:9730:9730 [5] NCCL INFO comm 0x7f2ddcedb110 rank 13 nranks 16 cudaDev 5 busId b6000 - Destroy COMPLETE
e2b29d3263f2:9728:9728 [3] NCCL INFO comm 0x7f700cedc6f0 rank 11 nranks 16 cudaDev 3 busId 20000 - Destroy COMPLETE
e2b29d3263f2:9729:9729 [4] NCCL INFO comm 0x7f090cedd090 rank 12 nranks 16 cudaDev 4 busId b5000 - Destroy COMPLETE
e2b29d3263f2:9726:9726 [1] NCCL INFO comm 0x7f327cede180 rank 9 nranks 16 cudaDev 1 busId f000 - Destroy COMPLETE
e2b29d3263f2:9731:9731 [6] NCCL INFO comm 0x55ee16369d30 rank 14 nranks 16 cudaDev 6 busId ce000 - Destroy COMPLETE
e2b29d3263f2:9732:9732 [7] NCCL INFO comm 0x56143182ef60 rank 15 nranks 16 cudaDev 7 busId cf000 - Destroy COMPLETE
e2b29d3263f2:9729:9729 [4] NCCL INFO comm 0x55b0187aa400 rank 12 nranks 16 cudaDev 4 busId b5000 - Destroy COMPLETE
e2b29d3263f2:9727:9727 [2] NCCL INFO comm 0x56340e90b430 rank 10 nranks 16 cudaDev 2 busId 1f000 - Destroy COMPLETE
e2b29d3263f2:9728:9728 [3] NCCL INFO comm 0x55f8bcaa8150 rank 11 nranks 16 cudaDev 3 busId 20000 - Destroy COMPLETE
e2b29d3263f2:9730:9730 [5] NCCL INFO comm 0x55a093a27fe0 rank 13 nranks 16 cudaDev 5 busId b6000 - Destroy COMPLETE
e2b29d3263f2:9726:9726 [1] NCCL INFO comm 0x563d5b061bb0 rank 9 nranks 16 cudaDev 1 busId f000 - Destroy COMPLETE
