[2025-08-18 09:56:03,873] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-08-18 09:56:06] llamafactory.cli:143 >> Initializing 8 distributed tasks at: 192.168.0.100:29500
[2025-08-18 09:56:13,909] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:13,968] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:13,980] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:13,993] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:14,072] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:14,119] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:14,149] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-18 09:56:14,150] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-18 09:56:15,358] [INFO] [comm.py:669:init_distributed] cdb=None
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-18 09:56:15,423] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-18 09:56:15,424] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-18 09:56:15,425] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-18 09:56:15,438] [INFO] [comm.py:669:init_distributed] cdb=None
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-18 09:56:15,518] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-18 09:56:15,622] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-18 09:56:15,648] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-18 09:56:15,650] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,417 >> loading file chat_template.jinja
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 6, world size: 8, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-18 09:56:16] llamafactory.hparams.parser:410 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2336] 2025-08-18 09:56:16,700 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:750] 2025-08-18 09:56:16,701 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-18 09:56:16,702 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-18 09:56:16,703 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2336] 2025-08-18 09:56:16,971 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-08-18 09:56:16] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.
[WARNING|2025-08-18 09:56:16] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-18 09:56:16] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[WARNING|2025-08-18 09:56:16] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-18 09:56:16] llamafactory.data.loader:143 >> Loading dataset identity.json...
[rank5]:[W818 09:56:17.481109675 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W818 09:56:17.495631289 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W818 09:56:17.497894514 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W818 09:56:17.509565859 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W818 09:56:17.523013338 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W818 09:56:17.523201130 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W818 09:56:17.544694089 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0% 0/91 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  33% 30/91 [00:00<00:00, 266.92 examples/s]Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 450.62 examples/s]
[INFO|2025-08-18 09:56:18] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...
Converting format of dataset (num_proc=16):   0% 0/999 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  88% 875/999 [00:00<00:00, 8676.20 examples/s]Converting format of dataset (num_proc=16): 100% 999/999 [00:00<00:00, 5612.77 examples/s]
[rank0]:[W818 09:56:19.533902319 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
6d774bbf43d0:2208:2208 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2208:2208 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2208:2208 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2208:2208 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2208:2208 [0] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2208:2208 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
6d774bbf43d0:2208:2208 [0] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2210:2210 [2] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2210:2210 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2210:2210 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2210:2210 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2210:2210 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2210:2210 [2] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2210:2210 [2] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2211:2211 [3] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2211:2211 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2211:2211 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2211:2211 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2211:2211 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2211:2211 [3] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2211:2211 [3] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2209:2209 [1] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2209:2209 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2209:2209 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2209:2209 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2209:2209 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2209:2209 [1] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2209:2209 [1] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2215:2215 [7] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2215:2215 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2215:2215 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2215:2215 [7] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2215:2215 [7] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2215:2215 [7] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2213:2213 [5] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2213:2213 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2213:2213 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2213:2213 [5] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2213:2213 [5] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2213:2213 [5] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2215:2215 [7] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2214:2214 [6] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2214:2214 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2214:2214 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2214:2214 [6] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2214:2214 [6] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2214:2214 [6] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2212:2212 [4] NCCL INFO cudaDriverVersion 12040
6d774bbf43d0:2212:2212 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2212:2212 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
6d774bbf43d0:2213:2213 [5] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2212:2212 [4] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
6d774bbf43d0:2212:2212 [4] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
6d774bbf43d0:2212:2212 [4] NCCL INFO NET/Plugin: Using internal network plugin.
6d774bbf43d0:2214:2214 [6] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2212:2212 [4] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2208:3378 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2208:3378 [0] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2208:3378 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2208:3378 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2208:3378 [0] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2208:3378 [0] NCCL INFO Using network Socket
6d774bbf43d0:2210:3379 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2210:3379 [2] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2210:3379 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2210:3379 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2210:3379 [2] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2210:3379 [2] NCCL INFO Using network Socket
6d774bbf43d0:2211:3380 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2211:3380 [3] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2211:3380 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2211:3380 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2211:3380 [3] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2211:3380 [3] NCCL INFO Using network Socket
6d774bbf43d0:2209:3381 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2209:3381 [1] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2209:3381 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2209:3381 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2209:3381 [1] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2209:3381 [1] NCCL INFO Using network Socket
6d774bbf43d0:2215:3382 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2215:3382 [7] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2215:3382 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2215:3382 [7] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2215:3382 [7] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2215:3382 [7] NCCL INFO Using network Socket
6d774bbf43d0:2212:3385 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2213:3383 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2214:3384 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
6d774bbf43d0:2212:3385 [4] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2212:3385 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2213:3383 [5] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2214:3384 [6] NCCL INFO Failed to open libibverbs.so[.1]
6d774bbf43d0:2214:3384 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2213:3383 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
6d774bbf43d0:2213:3383 [5] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2212:3385 [4] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2214:3384 [6] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
6d774bbf43d0:2213:3383 [5] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2213:3383 [5] NCCL INFO Using network Socket
6d774bbf43d0:2212:3385 [4] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2214:3384 [6] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2212:3385 [4] NCCL INFO Using network Socket
6d774bbf43d0:2214:3384 [6] NCCL INFO Using network Socket
6d774bbf43d0:2214:3384 [6] NCCL INFO ncclCommInitRank comm 0x55deecef0780 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId ce000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2213:3383 [5] NCCL INFO ncclCommInitRank comm 0x55a3e3a4d9a0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b6000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2211:3380 [3] NCCL INFO ncclCommInitRank comm 0x55ab7dd8ccf0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 20000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2212:3385 [4] NCCL INFO ncclCommInitRank comm 0x55c9b9908680 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b5000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2215:3382 [7] NCCL INFO ncclCommInitRank comm 0x55cd41ed6610 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId cf000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2209:3381 [1] NCCL INFO ncclCommInitRank comm 0x55bc832dcfc0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId f000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2210:3379 [2] NCCL INFO ncclCommInitRank comm 0x55a2f19bc780 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2208:3378 [0] NCCL INFO ncclCommInitRank comm 0x56437c43c380 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId e000 commId 0xc7832021894db689 - Init START
6d774bbf43d0:2209:3381 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff
6d774bbf43d0:2209:3381 [1] NCCL INFO NVLS multicast support is not available on dev 1
6d774bbf43d0:2215:3382 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000
6d774bbf43d0:2215:3382 [7] NCCL INFO NVLS multicast support is not available on dev 7
6d774bbf43d0:2212:3385 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000
6d774bbf43d0:2212:3385 [4] NCCL INFO NVLS multicast support is not available on dev 4
6d774bbf43d0:2211:3380 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
6d774bbf43d0:2211:3380 [3] NCCL INFO NVLS multicast support is not available on dev 3
6d774bbf43d0:2208:3378 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
6d774bbf43d0:2213:3383 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000
6d774bbf43d0:2213:3383 [5] NCCL INFO NVLS multicast support is not available on dev 5
6d774bbf43d0:2208:3378 [0] NCCL INFO NVLS multicast support is not available on dev 0
6d774bbf43d0:2210:3379 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
6d774bbf43d0:2210:3379 [2] NCCL INFO NVLS multicast support is not available on dev 2
6d774bbf43d0:2214:3384 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000
6d774bbf43d0:2214:3384 [6] NCCL INFO NVLS multicast support is not available on dev 6
6d774bbf43d0:2214:3384 [6] NCCL INFO comm 0x55deecef0780 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
6d774bbf43d0:2213:3383 [5] NCCL INFO comm 0x55a3e3a4d9a0 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
6d774bbf43d0:2210:3379 [2] NCCL INFO comm 0x55a2f19bc780 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
6d774bbf43d0:2209:3381 [1] NCCL INFO comm 0x55bc832dcfc0 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
6d774bbf43d0:2208:3378 [0] NCCL INFO comm 0x56437c43c380 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
6d774bbf43d0:2211:3380 [3] NCCL INFO comm 0x55ab7dd8ccf0 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
6d774bbf43d0:2214:3384 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2213:3383 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2214:3384 [6] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2215:3382 [7] NCCL INFO comm 0x55cd41ed6610 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
6d774bbf43d0:2213:3383 [5] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2210:3379 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
6d774bbf43d0:2209:3381 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
6d774bbf43d0:2212:3385 [4] NCCL INFO comm 0x55c9b9908680 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2211:3380 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2210:3379 [2] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2209:3381 [1] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2211:3380 [3] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2215:3382 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2212:3385 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3378 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
6d774bbf43d0:2208:3378 [0] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2215:3382 [7] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2212:3385 [4] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Connected all rings
6d774bbf43d0:2215:3382 [7] NCCL INFO Connected all rings
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Connected all rings
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3382 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Connected all rings
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Connected all rings
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Connected all rings
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Connected all rings
6d774bbf43d0:2212:3385 [4] NCCL INFO Connected all rings
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3384 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3379 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3381 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3385 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3380 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3383 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2208:3378 [0] NCCL INFO Connected all trees
6d774bbf43d0:2208:3378 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2208:3378 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2209:3381 [1] NCCL INFO Connected all trees
6d774bbf43d0:2209:3381 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2209:3381 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2210:3379 [2] NCCL INFO Connected all trees
6d774bbf43d0:2210:3379 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2210:3379 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2211:3380 [3] NCCL INFO Connected all trees
6d774bbf43d0:2211:3380 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2211:3380 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2212:3385 [4] NCCL INFO Connected all trees
6d774bbf43d0:2212:3385 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2212:3385 [4] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2213:3383 [5] NCCL INFO Connected all trees
6d774bbf43d0:2213:3383 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2213:3383 [5] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2214:3384 [6] NCCL INFO Connected all trees
6d774bbf43d0:2215:3382 [7] NCCL INFO Connected all trees
6d774bbf43d0:2214:3384 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2214:3384 [6] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2215:3382 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2215:3382 [7] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2215:3382 [7] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2215:3382 [7] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2215:3382 [7] NCCL INFO ncclCommInitRank comm 0x55cd41ed6610 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId cf000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2211:3380 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2211:3380 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2211:3380 [3] NCCL INFO ncclCommInitRank comm 0x55ab7dd8ccf0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 20000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2214:3384 [6] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2214:3384 [6] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2214:3384 [6] NCCL INFO ncclCommInitRank comm 0x55deecef0780 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId ce000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2212:3385 [4] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2212:3385 [4] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2212:3385 [4] NCCL INFO ncclCommInitRank comm 0x55c9b9908680 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b5000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2210:3379 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2210:3379 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2210:3379 [2] NCCL INFO ncclCommInitRank comm 0x55a2f19bc780 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2208:3378 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2208:3378 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2208:3378 [0] NCCL INFO ncclCommInitRank comm 0x56437c43c380 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId e000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2213:3383 [5] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2213:3383 [5] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2213:3383 [5] NCCL INFO ncclCommInitRank comm 0x55a3e3a4d9a0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b6000 commId 0xc7832021894db689 - Init COMPLETE
6d774bbf43d0:2209:3381 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
6d774bbf43d0:2209:3381 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
6d774bbf43d0:2209:3381 [1] NCCL INFO ncclCommInitRank comm 0x55bc832dcfc0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId f000 commId 0xc7832021894db689 - Init COMPLETE
Running tokenizer on dataset (num_proc=16):   0% 0/1090 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6% 69/1090 [00:00<00:07, 135.01 examples/s]Running tokenizer on dataset (num_proc=16):  19% 206/1090 [00:00<00:02, 355.42 examples/s]Running tokenizer on dataset (num_proc=16):  38% 410/1090 [00:00<00:01, 648.42 examples/s]Running tokenizer on dataset (num_proc=16):  50% 546/1090 [00:00<00:00, 715.55 examples/s]Running tokenizer on dataset (num_proc=16):  63% 682/1090 [00:01<00:00, 772.03 examples/s]Running tokenizer on dataset (num_proc=16):  75% 818/1090 [00:01<00:00, 774.16 examples/s]Running tokenizer on dataset (num_proc=16):  88% 954/1090 [00:01<00:00, 823.35 examples/s]Running tokenizer on dataset (num_proc=16): 100% 1090/1090 [00:01<00:00, 930.44 examples/s]Running tokenizer on dataset (num_proc=16): 100% 1090/1090 [00:01<00:00, 671.07 examples/s]
training example:
input_ids:
[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91, 408, 8757, 842, 91, 1339, 6023, 151665, 27, 91, 2468, 8757, 842, 91, 29, 77091, 27, 91, 408, 8757, 842, 91, 1339, 9707, 0, 358, 1079, 5867, 606, 38154, 458, 15235, 17847, 7881, 553, 5867, 3094, 3417, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151665]
inputs:
<|start_header_id|>user<|end_header_id|>

hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9707, 0, 358, 1079, 5867, 606, 38154, 458, 15235, 17847, 7881, 553, 5867, 3094, 3417, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151665]
labels:
Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>
[INFO|configuration_utils.py:750] 2025-08-18 09:56:24,394 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-18 09:56:24,395 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-08-18 09:56:24] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1305] 2025-08-18 09:56:24,502 >> loading weights file /data/Qwen2.5-32B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:4363] 2025-08-18 09:56:24,502 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-08-18 09:56:24,502] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[INFO|configuration_utils.py:1098] 2025-08-18 09:56:24,513 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[2025-08-18 09:56:24,615] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,616] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,616] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,617] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,617] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,619] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:24,626] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:25,382] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.59it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.58it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.55it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.59it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:09,  1.60it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.60it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:10,  1.60it/s]Loading checkpoint shards:   6% 1/17 [00:00<00:11,  1.35it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.48it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.48it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.49it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.49it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.47it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.49it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.49it/s]Loading checkpoint shards:  12% 2/17 [00:01<00:10,  1.40it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.46it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.47it/s]Loading checkpoint shards:  18% 3/17 [00:02<00:09,  1.41it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:08,  1.45it/s]Loading checkpoint shards:  24% 4/17 [00:02<00:09,  1.43it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.45it/s]Loading checkpoint shards:  29% 5/17 [00:03<00:08,  1.43it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.44it/s]Loading checkpoint shards:  35% 6/17 [00:04<00:07,  1.43it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.44it/s]Loading checkpoint shards:  41% 7/17 [00:04<00:06,  1.43it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.44it/s]Loading checkpoint shards:  47% 8/17 [00:05<00:06,  1.43it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  53% 9/17 [00:06<00:05,  1.45it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  59% 10/17 [00:06<00:04,  1.46it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  65% 11/17 [00:07<00:04,  1.47it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.47it/s]Loading checkpoint shards:  71% 12/17 [00:08<00:03,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  76% 13/17 [00:08<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  82% 14/17 [00:09<00:02,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.48it/s]Loading checkpoint shards:  88% 15/17 [00:10<00:01,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards:  94% 16/17 [00:10<00:00,  1.49it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]
Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]
Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]
Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]

Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.88it/s]
Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.53it/s]
Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.60it/s]Loading checkpoint shards: 100% 17/17 [00:11<00:00,  1.48it/s]
[INFO|modeling_utils.py:5606] 2025-08-18 09:56:36,913 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:5614] 2025-08-18 09:56:36,913 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data/Qwen2.5-32B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-08-18 09:56:36,917 >> loading configuration file /data/Qwen2.5-32B-Instruct/generation_config.json
[INFO|configuration_utils.py:1098] 2025-08-18 09:56:36,918 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-08-18 09:56:36] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-18 09:56:36] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-18 09:56:36] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-08-18 09:56:36] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-08-18 09:56:36] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,o_proj,k_proj,up_proj,down_proj,q_proj,v_proj
[INFO|2025-08-18 09:56:38] llamafactory.model.loader:143 >> trainable params: 67,108,864 || all params: 32,830,985,216 || trainable%: 0.2044
[INFO|trainer.py:757] 2025-08-18 09:56:38,145 >> Using auto half precision backend
[WARNING|2025-08-18 09:56:38] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[2025-08-18 09:56:38,500] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-08-18 09:56:38,500] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-08-18 09:56:38,559] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-18 09:56:38,565] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-08-18 09:56:38,565] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-18 09:56:38,660] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-08-18 09:56:38,660] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-08-18 09:56:38,660] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-08-18 09:56:38,660] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-08-18 09:56:38,859] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-08-18 09:56:38,859] [INFO] [utils.py:782:see_memory_usage] MA 7.75 GB         Max_MA 11.8 GB         CA 7.89 GB         Max_CA 12 GB 
[2025-08-18 09:56:38,859] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.32 GB, percent = 2.5%
[2025-08-18 09:56:38,871] [INFO] [stage3.py:170:__init__] Reduce bucket size 26214400
[2025-08-18 09:56:38,871] [INFO] [stage3.py:171:__init__] Prefetch bucket size 23592960
[2025-08-18 09:56:39,079] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-08-18 09:56:39,079] [INFO] [utils.py:782:see_memory_usage] MA 7.75 GB         Max_MA 7.75 GB         CA 7.89 GB         Max_CA 8 GB 
[2025-08-18 09:56:39,079] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.43 GB, percent = 2.5%
Parameter Offload: Total persistent parameters: 25760768 in 1025 params
[2025-08-18 09:56:40,464] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-08-18 09:56:40,464] [INFO] [utils.py:782:see_memory_usage] MA 7.64 GB         Max_MA 7.75 GB         CA 7.89 GB         Max_CA 8 GB 
[2025-08-18 09:56:40,464] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.75 GB, percent = 2.6%
[2025-08-18 09:56:40,743] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-08-18 09:56:40,743] [INFO] [utils.py:782:see_memory_usage] MA 7.64 GB         Max_MA 7.64 GB         CA 7.89 GB         Max_CA 8 GB 
[2025-08-18 09:56:40,743] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.75 GB, percent = 2.6%
[2025-08-18 09:56:41,411] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-08-18 09:56:41,412] [INFO] [utils.py:782:see_memory_usage] MA 7.64 GB         Max_MA 7.64 GB         CA 7.66 GB         Max_CA 8 GB 
[2025-08-18 09:56:41,412] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.87 GB, percent = 2.6%
[2025-08-18 09:56:41,716] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-08-18 09:56:41,716] [INFO] [utils.py:782:see_memory_usage] MA 7.64 GB         Max_MA 7.64 GB         CA 7.66 GB         Max_CA 8 GB 
[2025-08-18 09:56:41,716] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.87 GB, percent = 2.6%
[2025-08-18 09:56:42,000] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-08-18 09:56:42,000] [INFO] [utils.py:782:see_memory_usage] MA 7.68 GB         Max_MA 7.69 GB         CA 7.7 GB         Max_CA 8 GB 
[2025-08-18 09:56:42,000] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.87 GB, percent = 2.6%
[2025-08-18 09:56:42,284] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-18 09:56:42,284] [INFO] [utils.py:782:see_memory_usage] MA 7.68 GB         Max_MA 7.68 GB         CA 7.7 GB         Max_CA 8 GB 
[2025-08-18 09:56:42,284] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.87 GB, percent = 2.6%
[2025-08-18 09:56:42,578] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-18 09:56:42,579] [INFO] [utils.py:782:see_memory_usage] MA 7.68 GB         Max_MA 7.71 GB         CA 7.74 GB         Max_CA 8 GB 
[2025-08-18 09:56:42,579] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.87 GB, percent = 2.6%
[2025-08-18 09:56:42,579] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-08-18 09:56:43,240] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-18 09:56:43,241] [INFO] [utils.py:782:see_memory_usage] MA 7.74 GB         Max_MA 7.74 GB         CA 7.76 GB         Max_CA 8 GB 
[2025-08-18 09:56:43,241] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.98 GB, percent = 2.7%
[2025-08-18 09:56:43,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-08-18 09:56:43,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-18 09:56:43,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-18 09:56:43,241] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-08-18 09:56:43,254] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f060c16bc50>
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-08-18 09:56:43,255] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   world_size ................... 8
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-08-18 09:56:43,256] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-18 09:56:43,257] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-08-18 09:56:43,257] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2433] 2025-08-18 09:56:43,258 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-18 09:56:43,258 >>   Num examples = 1,090
[INFO|trainer.py:2435] 2025-08-18 09:56:43,258 >>   Num Epochs = 3
[INFO|trainer.py:2436] 2025-08-18 09:56:43,258 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2439] 2025-08-18 09:56:43,258 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2440] 2025-08-18 09:56:43,258 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-18 09:56:43,258 >>   Total optimization steps = 54
[INFO|trainer.py:2442] 2025-08-18 09:56:43,267 >>   Number of trainable parameters = 67,108,864
  0% 0/54 [00:00<?, ?it/s]6d774bbf43d0:2208:3792 [0] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2208:3801 [0] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2208:3801 [0] NCCL INFO Using network Socket
6d774bbf43d0:2212:3785 [4] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2215:3767 [7] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2209:3749 [1] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2211:3782 [3] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2210:3746 [2] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2213:3755 [5] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2214:3774 [6] NCCL INFO Comm config Blocking set to 1
6d774bbf43d0:2213:3806 [5] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2213:3806 [5] NCCL INFO Using network Socket
6d774bbf43d0:2211:3805 [3] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2211:3805 [3] NCCL INFO Using network Socket
6d774bbf43d0:2210:3807 [2] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2210:3807 [2] NCCL INFO Using network Socket
6d774bbf43d0:2214:3808 [6] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2214:3808 [6] NCCL INFO Using network Socket
6d774bbf43d0:2212:3802 [4] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2212:3802 [4] NCCL INFO Using network Socket
6d774bbf43d0:2215:3803 [7] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2215:3803 [7] NCCL INFO Using network Socket
6d774bbf43d0:2209:3804 [1] NCCL INFO Using non-device net plugin version 0
6d774bbf43d0:2209:3804 [1] NCCL INFO Using network Socket
6d774bbf43d0:2214:3808 [6] NCCL INFO ncclCommInitRank comm 0x7fca186e01d0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId ce000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2211:3805 [3] NCCL INFO ncclCommInitRank comm 0x7f5cac9df360 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 20000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2213:3806 [5] NCCL INFO ncclCommInitRank comm 0x7fb8444b42e0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b6000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2215:3803 [7] NCCL INFO ncclCommInitRank comm 0x7f20604ba700 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId cf000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2210:3807 [2] NCCL INFO ncclCommInitRank comm 0x7fa54c7b64f0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 1f000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2208:3801 [0] NCCL INFO ncclCommInitRank comm 0x7f054ca982d0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId e000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2209:3804 [1] NCCL INFO ncclCommInitRank comm 0x7fe17c4b5510 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId f000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2212:3802 [4] NCCL INFO ncclCommInitRank comm 0x7f9f4c8605f0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b5000 commId 0x3495b6ed6c813d85 - Init START
6d774bbf43d0:2214:3808 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000
6d774bbf43d0:2214:3808 [6] NCCL INFO NVLS multicast support is not available on dev 6
6d774bbf43d0:2210:3807 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
6d774bbf43d0:2210:3807 [2] NCCL INFO NVLS multicast support is not available on dev 2
6d774bbf43d0:2213:3806 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000
6d774bbf43d0:2213:3806 [5] NCCL INFO NVLS multicast support is not available on dev 5
6d774bbf43d0:2208:3801 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
6d774bbf43d0:2208:3801 [0] NCCL INFO NVLS multicast support is not available on dev 0
6d774bbf43d0:2211:3805 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
6d774bbf43d0:2211:3805 [3] NCCL INFO NVLS multicast support is not available on dev 3
6d774bbf43d0:2209:3804 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff
6d774bbf43d0:2209:3804 [1] NCCL INFO NVLS multicast support is not available on dev 1
6d774bbf43d0:2212:3802 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000
6d774bbf43d0:2212:3802 [4] NCCL INFO NVLS multicast support is not available on dev 4
6d774bbf43d0:2215:3803 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000
6d774bbf43d0:2215:3803 [7] NCCL INFO NVLS multicast support is not available on dev 7
6d774bbf43d0:2214:3808 [6] NCCL INFO comm 0x7fca186e01d0 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
6d774bbf43d0:2210:3807 [2] NCCL INFO comm 0x7fa54c7b64f0 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
6d774bbf43d0:2213:3806 [5] NCCL INFO comm 0x7fb8444b42e0 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
6d774bbf43d0:2209:3804 [1] NCCL INFO comm 0x7fe17c4b5510 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
6d774bbf43d0:2212:3802 [4] NCCL INFO comm 0x7f9f4c8605f0 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
6d774bbf43d0:2211:3805 [3] NCCL INFO comm 0x7f5cac9df360 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
6d774bbf43d0:2208:3801 [0] NCCL INFO comm 0x7f054ca982d0 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
6d774bbf43d0:2215:3803 [7] NCCL INFO comm 0x7f20604ba700 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
6d774bbf43d0:2214:3808 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2214:3808 [6] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2210:3807 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
6d774bbf43d0:2213:3806 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2209:3804 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
6d774bbf43d0:2213:3806 [5] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2209:3804 [1] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
6d774bbf43d0:2208:3801 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
6d774bbf43d0:2208:3801 [0] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2210:3807 [2] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2211:3805 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
6d774bbf43d0:2211:3805 [3] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2215:3803 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
6d774bbf43d0:2215:3803 [7] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2212:3802 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
6d774bbf43d0:2212:3802 [4] NCCL INFO P2P Chunksize set to 524288
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Connected all rings
6d774bbf43d0:2209:3804 [1] NCCL INFO Connected all rings
6d774bbf43d0:2208:3801 [0] NCCL INFO Connected all rings
6d774bbf43d0:2212:3802 [4] NCCL INFO Connected all rings
6d774bbf43d0:2211:3805 [3] NCCL INFO Connected all rings
6d774bbf43d0:2215:3803 [7] NCCL INFO Connected all rings
6d774bbf43d0:2213:3806 [5] NCCL INFO Connected all rings
6d774bbf43d0:2214:3808 [6] NCCL INFO Connected all rings
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2215:3803 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2210:3807 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2212:3802 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2209:3804 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2211:3805 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2214:3808 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2213:3806 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/CUMEM/read
6d774bbf43d0:2208:3801 [0] NCCL INFO Connected all trees
6d774bbf43d0:2208:3801 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2208:3801 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2215:3803 [7] NCCL INFO Connected all trees
6d774bbf43d0:2215:3803 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2215:3803 [7] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2214:3808 [6] NCCL INFO Connected all trees
6d774bbf43d0:2214:3808 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2214:3808 [6] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2210:3807 [2] NCCL INFO Connected all trees
6d774bbf43d0:2210:3807 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2210:3807 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2209:3804 [1] NCCL INFO Connected all trees
6d774bbf43d0:2209:3804 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2209:3804 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2211:3805 [3] NCCL INFO Connected all trees
6d774bbf43d0:2213:3806 [5] NCCL INFO Connected all trees
6d774bbf43d0:2213:3806 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2213:3806 [5] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2212:3802 [4] NCCL INFO Connected all trees
6d774bbf43d0:2211:3805 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2211:3805 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2212:3802 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
6d774bbf43d0:2212:3802 [4] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
6d774bbf43d0:2214:3808 [6] NCCL INFO ncclCommInitRank comm 0x7fca186e01d0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId ce000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2210:3807 [2] NCCL INFO ncclCommInitRank comm 0x7fa54c7b64f0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 1f000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2213:3806 [5] NCCL INFO ncclCommInitRank comm 0x7fb8444b42e0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b6000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2209:3804 [1] NCCL INFO ncclCommInitRank comm 0x7fe17c4b5510 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId f000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2212:3802 [4] NCCL INFO ncclCommInitRank comm 0x7f9f4c8605f0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId b5000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2208:3801 [0] NCCL INFO ncclCommInitRank comm 0x7f054ca982d0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId e000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2215:3803 [7] NCCL INFO ncclCommInitRank comm 0x7f20604ba700 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId cf000 commId 0x3495b6ed6c813d85 - Init COMPLETE
6d774bbf43d0:2211:3805 [3] NCCL INFO ncclCommInitRank comm 0x7f5cac9df360 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 20000 commId 0x3495b6ed6c813d85 - Init COMPLETE
  2% 1/54 [00:29<26:04, 29.52s/it]  4% 2/54 [00:48<19:58, 23.06s/it]  6% 3/54 [01:06<17:49, 20.98s/it]  7% 4/54 [01:25<16:41, 20.02s/it]  9% 5/54 [01:43<15:54, 19.48s/it] 11% 6/54 [02:02<15:18, 19.14s/it] 13% 7/54 [02:20<14:50, 18.94s/it] 15% 8/54 [02:39<14:24, 18.79s/it] 17% 9/54 [02:57<14:01, 18.69s/it] 19% 10/54 [03:16<13:39, 18.63s/it]                                   {'loss': 1.1022, 'grad_norm': 0.10100690814166381, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.58}
 19% 10/54 [03:16<13:39, 18.63s/it] 20% 11/54 [03:34<13:19, 18.58s/it] 22% 12/54 [03:54<13:13, 18.90s/it] 24% 13/54 [04:12<12:50, 18.80s/it] 26% 14/54 [04:31<12:28, 18.71s/it] 28% 15/54 [04:49<12:07, 18.66s/it] 30% 16/54 [05:08<11:47, 18.63s/it] 31% 17/54 [05:26<11:28, 18.60s/it] 33% 18/54 [05:29<08:13, 13.71s/it] 35% 19/54 [05:47<08:53, 15.23s/it] 37% 20/54 [06:06<09:10, 16.19s/it]                                   {'loss': 1.0154, 'grad_norm': 0.13059649018988403, 'learning_rate': 8.296729075500344e-05, 'epoch': 1.12}
 37% 20/54 [06:06<09:10, 16.19s/it] 39% 21/54 [06:24<09:17, 16.88s/it] 41% 22/54 [06:43<09:16, 17.38s/it] 43% 23/54 [07:01<09:08, 17.70s/it] 44% 24/54 [07:20<08:57, 17.93s/it] 46% 25/54 [07:39<08:54, 18.44s/it] 48% 26/54 [07:58<08:37, 18.47s/it] 50% 27/54 [08:17<08:20, 18.52s/it] 52% 28/54 [08:35<08:02, 18.54s/it] 54% 29/54 [08:54<07:43, 18.55s/it] 56% 30/54 [09:12<07:25, 18.54s/it]                                   {'loss': 0.9576, 'grad_norm': 0.08865673163334672, 'learning_rate': 5.327015646150716e-05, 'epoch': 1.7}
 56% 30/54 [09:12<07:25, 18.54s/it] 57% 31/54 [09:31<07:07, 18.57s/it] 59% 32/54 [09:50<06:48, 18.58s/it] 61% 33/54 [10:08<06:30, 18.59s/it] 63% 34/54 [10:27<06:12, 18.60s/it] 65% 35/54 [10:45<05:53, 18.61s/it] 67% 36/54 [10:48<04:07, 13.74s/it] 69% 37/54 [11:07<04:19, 15.25s/it] 70% 38/54 [11:26<04:22, 16.38s/it] 72% 39/54 [11:45<04:17, 17.16s/it] 74% 40/54 [12:03<04:06, 17.60s/it]                                   {'loss': 0.9819, 'grad_norm': 0.09655833832027064, 'learning_rate': 2.2221488349019903e-05, 'epoch': 2.23}
 74% 40/54 [12:03<04:06, 17.60s/it] 76% 41/54 [12:22<03:52, 17.90s/it] 78% 42/54 [12:40<03:37, 18.12s/it] 80% 43/54 [12:59<03:21, 18.27s/it] 81% 44/54 [13:18<03:03, 18.37s/it] 83% 45/54 [13:36<02:46, 18.45s/it] 85% 46/54 [13:55<02:28, 18.51s/it] 87% 47/54 [14:14<02:09, 18.55s/it] 89% 48/54 [14:32<01:51, 18.58s/it] 91% 49/54 [14:51<01:32, 18.59s/it] 93% 50/54 [15:09<01:14, 18.60s/it]                                   {'loss': 0.9365, 'grad_norm': 0.08675964959317668, 'learning_rate': 2.653493525244721e-06, 'epoch': 2.82}
 93% 50/54 [15:09<01:14, 18.60s/it] 94% 51/54 [15:29<00:56, 18.79s/it] 96% 52/54 [15:48<00:37, 18.93s/it] 98% 53/54 [16:07<00:18, 18.84s/it]100% 54/54 [16:09<00:00, 13.91s/it][INFO|trainer.py:4074] 2025-08-18 10:13:23,189 >> Saving model checkpoint to /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54
[INFO|configuration_utils.py:750] 2025-08-18 10:13:23,208 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-18 10:13:23,209 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2393] 2025-08-18 10:13:23,373 >> chat template saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-18 10:13:23,373 >> tokenizer config file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-18 10:13:23,373 >> Special tokens file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/special_tokens_map.json
[2025-08-18 10:13:25,147] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step51 is about to be saved!
[2025-08-18 10:13:25,189] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-08-18 10:13:25,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-08-18 10:13:25,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-08-18 10:13:25,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-08-18 10:13:25,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-08-18 10:13:25,324] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/checkpoint-54/global_step51/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-08-18 10:13:25,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step51 is ready now!
[INFO|trainer.py:2718] 2025-08-18 10:13:25,405 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                   {'train_runtime': 1002.1385, 'train_samples_per_second': 3.263, 'train_steps_per_second': 0.054, 'train_loss': 0.9759573627401281, 'epoch': 3.0}
100% 54/54 [16:42<00:00, 13.91s/it]100% 54/54 [16:42<00:00, 18.56s/it]
[INFO|trainer.py:4074] 2025-08-18 10:13:55,507 >> Saving model checkpoint to /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/
[INFO|configuration_utils.py:750] 2025-08-18 10:13:55,518 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-18 10:13:55,518 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2393] 2025-08-18 10:13:55,728 >> chat template saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-18 10:13:55,729 >> tokenizer config file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-18 10:13:55,729 >> Special tokens file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/special_tokens_map.json
6d774bbf43d0:2210:2210 [2] NCCL INFO comm 0x7fa54c7b64f0 rank 2 nranks 8 cudaDev 2 busId 1f000 - Destroy COMPLETE
6d774bbf43d0:2213:2213 [5] NCCL INFO comm 0x7fb8444b42e0 rank 5 nranks 8 cudaDev 5 busId b6000 - Destroy COMPLETE
6d774bbf43d0:2214:2214 [6] NCCL INFO comm 0x7fca186e01d0 rank 6 nranks 8 cudaDev 6 busId ce000 - Destroy COMPLETE
6d774bbf43d0:2209:2209 [1] NCCL INFO comm 0x7fe17c4b5510 rank 1 nranks 8 cudaDev 1 busId f000 - Destroy COMPLETE
6d774bbf43d0:2215:2215 [7] NCCL INFO comm 0x7f20604ba700 rank 7 nranks 8 cudaDev 7 busId cf000 - Destroy COMPLETE
6d774bbf43d0:2212:2212 [4] NCCL INFO comm 0x7f9f4c8605f0 rank 4 nranks 8 cudaDev 4 busId b5000 - Destroy COMPLETE
6d774bbf43d0:2211:2211 [3] NCCL INFO comm 0x7f5cac9df360 rank 3 nranks 8 cudaDev 3 busId 20000 - Destroy COMPLETE
6d774bbf43d0:2210:2210 [2] NCCL INFO comm 0x55a2f19bc780 rank 2 nranks 8 cudaDev 2 busId 1f000 - Destroy COMPLETE
***** train metrics *****
  epoch                    =        3.0
  total_flos               =    88716GF
  train_loss               =      0.976
  train_runtime            = 0:16:42.13
  train_samples_per_second =      3.263
  train_steps_per_second   =      0.054
6d774bbf43d0:2209:2209 [1] NCCL INFO comm 0x55bc832dcfc0 rank 1 nranks 8 cudaDev 1 busId f000 - Destroy COMPLETE
Figure saved at: /data/llamafactory/saves/qwen2_5vl-32b/lora/sft/training_loss.png
[WARNING|2025-08-18 10:13:57] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-08-18 10:13:57] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:456] 2025-08-18 10:13:57,532 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
6d774bbf43d0:2211:2211 [3] NCCL INFO comm 0x55ab7dd8ccf0 rank 3 nranks 8 cudaDev 3 busId 20000 - Destroy COMPLETE
6d774bbf43d0:2214:2214 [6] NCCL INFO comm 0x55deecef0780 rank 6 nranks 8 cudaDev 6 busId ce000 - Destroy COMPLETE
6d774bbf43d0:2215:2215 [7] NCCL INFO comm 0x55cd41ed6610 rank 7 nranks 8 cudaDev 7 busId cf000 - Destroy COMPLETE
6d774bbf43d0:2213:2213 [5] NCCL INFO comm 0x55a3e3a4d9a0 rank 5 nranks 8 cudaDev 5 busId b6000 - Destroy COMPLETE
6d774bbf43d0:2212:2212 [4] NCCL INFO comm 0x55c9b9908680 rank 4 nranks 8 cudaDev 4 busId b5000 - Destroy COMPLETE
6d774bbf43d0:2208:2208 [0] NCCL INFO comm 0x7f054ca982d0 rank 0 nranks 8 cudaDev 0 busId e000 - Destroy COMPLETE
6d774bbf43d0:2208:2208 [0] NCCL INFO comm 0x56437c43c380 rank 0 nranks 8 cudaDev 0 busId e000 - Destroy COMPLETE
