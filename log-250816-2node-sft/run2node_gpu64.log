[2025-08-15 16:51:46,788] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-08-15 16:51:49] llamafactory.cli:143 >> Initializing 8 distributed tasks at: 192.168.0.100:29500
[INFO|2025-08-15 16:51:49] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 2, node rank: 0
[2025-08-15 16:52:03,347] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,422] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,452] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,715] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,776] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,795] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,824] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-15 16:52:03,829] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-15 16:52:04,793] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:04,869] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:04,874] [INFO] [comm.py:669:init_distributed] cdb=None
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-15 16:52:05,169] [INFO] [comm.py:669:init_distributed] cdb=None
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-08-15 16:52:05,310] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:05,310] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:05,375] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:05,396] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-08-15 16:52:05,396] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2025-08-15 16:52:05] llamafactory.hparams.parser:410 >> Process rank: 3, world size: 16, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:05] llamafactory.hparams.parser:410 >> Process rank: 6, world size: 16, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:05] llamafactory.hparams.parser:410 >> Process rank: 5, world size: 16, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:05] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 16, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:05,910 >> loading file chat_template.jinja
[INFO|2025-08-15 16:52:05] llamafactory.hparams.parser:410 >> Process rank: 7, world size: 16, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:06] llamafactory.hparams.parser:410 >> Process rank: 2, world size: 16, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:06] llamafactory.hparams.parser:410 >> Process rank: 1, world size: 16, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-08-15 16:52:06] llamafactory.hparams.parser:410 >> Process rank: 4, world size: 16, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2336] 2025-08-15 16:52:06,183 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:750] 2025-08-15 16:52:06,183 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-15 16:52:06,185 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2065] 2025-08-15 16:52:06,185 >> loading file chat_template.jinja
[rank3]:[W815 16:52:06.795171180 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[INFO|tokenization_utils_base.py:2336] 2025-08-15 16:52:06,452 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-08-15 16:52:06] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.
[WARNING|2025-08-15 16:52:06] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-15 16:52:06] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[WARNING|2025-08-15 16:52:06] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-08-15 16:52:06] llamafactory.data.loader:143 >> Loading dataset identity.json...
[rank5]:[W815 16:52:06.850555837 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W815 16:52:06.862888284 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W815 16:52:06.929907387 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W815 16:52:06.957190152 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W815 16:52:06.958601138 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W815 16:52:06.959780736 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0% 0/91 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  40% 36/91 [00:00<00:00, 315.15 examples/s]Converting format of dataset (num_proc=16): 100% 91/91 [00:00<00:00, 443.04 examples/s]
[INFO|2025-08-15 16:52:07] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...
Converting format of dataset (num_proc=16):   0% 0/999 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  44% 441/999 [00:00<00:00, 4372.72 examples/s]Converting format of dataset (num_proc=16): 100% 999/999 [00:00<00:00, 5413.45 examples/s]
[rank0]:[W815 16:52:08.032346807 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
5a347196ee9c:14794:14794 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14794:14794 [0] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14794:14794 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14794:14794 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14794:14794 [0] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14794:14794 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
5a347196ee9c:14794:14794 [0] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14801:14801 [7] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14801:14801 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14801:14801 [7] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14801:14801 [7] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14801:14801 [7] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14801:14801 [7] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14801:14801 [7] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14798:14798 [4] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14798:14798 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14798:14798 [4] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14798:14798 [4] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14798:14798 [4] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14798:14798 [4] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14798:14798 [4] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14795:14795 [1] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14795:14795 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14795:14795 [1] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14795:14795 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14795:14795 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14795:14795 [1] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14795:14795 [1] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14797:14797 [3] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14797:14797 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14797:14797 [3] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14797:14797 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14797:14797 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14797:14797 [3] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14800:14800 [6] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14800:14800 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14800:14800 [6] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14800:14800 [6] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14800:14800 [6] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14800:14800 [6] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14797:14797 [3] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14796:14796 [2] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14796:14796 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14796:14796 [2] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14796:14796 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14796:14796 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14796:14796 [2] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14799:14799 [5] NCCL INFO cudaDriverVersion 12040
5a347196ee9c:14799:14799 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14799:14799 [5] NCCL INFO Bootstrap : Using eth0:192.168.0.100<0>
5a347196ee9c:14800:14800 [6] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14796:14796 [2] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14799:14799 [5] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
5a347196ee9c:14799:14799 [5] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
5a347196ee9c:14799:14799 [5] NCCL INFO NET/Plugin: Using internal network plugin.
5a347196ee9c:14799:14799 [5] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14794:16156 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14794:16156 [0] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14794:16156 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14794:16156 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14794:16156 [0] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14794:16156 [0] NCCL INFO Using network Socket
5a347196ee9c:14801:16157 [7] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14801:16157 [7] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14801:16157 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14801:16157 [7] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14801:16157 [7] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14801:16157 [7] NCCL INFO Using network Socket
5a347196ee9c:14798:16158 [4] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14798:16158 [4] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14798:16158 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14798:16158 [4] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14798:16158 [4] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14798:16158 [4] NCCL INFO Using network Socket
5a347196ee9c:14795:16159 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14795:16159 [1] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14795:16159 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14795:16159 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14795:16159 [1] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14795:16159 [1] NCCL INFO Using network Socket
5a347196ee9c:14797:16160 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14796:16162 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14797:16160 [3] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14797:16160 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14796:16162 [2] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14796:16162 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14796:16162 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14797:16160 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14796:16162 [2] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14797:16160 [3] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14796:16162 [2] NCCL INFO Using network Socket
5a347196ee9c:14797:16160 [3] NCCL INFO Using network Socket
5a347196ee9c:14800:16161 [6] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14800:16161 [6] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14800:16161 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14800:16161 [6] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14800:16161 [6] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14800:16161 [6] NCCL INFO Using network Socket
5a347196ee9c:14799:16163 [5] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
5a347196ee9c:14799:16163 [5] NCCL INFO Failed to open libibverbs.so[.1]
5a347196ee9c:14799:16163 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
5a347196ee9c:14799:16163 [5] NCCL INFO NET/Socket : Using [0]eth0:192.168.0.100<0>
5a347196ee9c:14799:16163 [5] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14799:16163 [5] NCCL INFO Using network Socket
5a347196ee9c:14801:16157 [7] NCCL INFO ncclCommInitRank comm 0x562054163700 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14800:16161 [6] NCCL INFO ncclCommInitRank comm 0x55af1a184950 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14797:16160 [3] NCCL INFO ncclCommInitRank comm 0x5651b78afc80 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14799:16163 [5] NCCL INFO ncclCommInitRank comm 0x55c5eea584c0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14798:16158 [4] NCCL INFO ncclCommInitRank comm 0x563c393acfe0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14796:16162 [2] NCCL INFO ncclCommInitRank comm 0x55ea0e49f520 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14794:16156 [0] NCCL INFO ncclCommInitRank comm 0x561e645335d0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14795:16159 [1] NCCL INFO ncclCommInitRank comm 0x55f175e018b0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xa0364583bc190008 - Init START
5a347196ee9c:14797:16160 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
5a347196ee9c:14797:16160 [3] NCCL INFO NVLS multicast support is not available on dev 3
5a347196ee9c:14801:16157 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000
5a347196ee9c:14801:16157 [7] NCCL INFO NVLS multicast support is not available on dev 7
5a347196ee9c:14798:16158 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000
5a347196ee9c:14796:16162 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
5a347196ee9c:14798:16158 [4] NCCL INFO NVLS multicast support is not available on dev 4
5a347196ee9c:14796:16162 [2] NCCL INFO NVLS multicast support is not available on dev 2
5a347196ee9c:14794:16156 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
5a347196ee9c:14794:16156 [0] NCCL INFO NVLS multicast support is not available on dev 0
5a347196ee9c:14795:16159 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff
5a347196ee9c:14795:16159 [1] NCCL INFO NVLS multicast support is not available on dev 1
5a347196ee9c:14799:16163 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000
5a347196ee9c:14799:16163 [5] NCCL INFO NVLS multicast support is not available on dev 5
5a347196ee9c:14800:16161 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000
5a347196ee9c:14800:16161 [6] NCCL INFO NVLS multicast support is not available on dev 6
5a347196ee9c:14800:16161 [6] NCCL INFO comm 0x55af1a184950 rank 6 nRanks 16 nNodes 2 localRanks 8 localRank 6 MNNVL 0
5a347196ee9c:14801:16157 [7] NCCL INFO comm 0x562054163700 rank 7 nRanks 16 nNodes 2 localRanks 8 localRank 7 MNNVL 0
5a347196ee9c:14800:16161 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
5a347196ee9c:14800:16161 [6] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14801:16157 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
5a347196ee9c:14801:16157 [7] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14795:16159 [1] NCCL INFO comm 0x55f175e018b0 rank 1 nRanks 16 nNodes 2 localRanks 8 localRank 1 MNNVL 0
5a347196ee9c:14794:16156 [0] NCCL INFO comm 0x561e645335d0 rank 0 nRanks 16 nNodes 2 localRanks 8 localRank 0 MNNVL 0
5a347196ee9c:14799:16163 [5] NCCL INFO comm 0x55c5eea584c0 rank 5 nRanks 16 nNodes 2 localRanks 8 localRank 5 MNNVL 0
5a347196ee9c:14799:16163 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/02 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
5a347196ee9c:14799:16163 [5] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14795:16159 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
5a347196ee9c:14796:16162 [2] NCCL INFO comm 0x55ea0e49f520 rank 2 nRanks 16 nNodes 2 localRanks 8 localRank 2 MNNVL 0
5a347196ee9c:14795:16159 [1] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/02 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
5a347196ee9c:14798:16158 [4] NCCL INFO comm 0x563c393acfe0 rank 4 nRanks 16 nNodes 2 localRanks 8 localRank 4 MNNVL 0
5a347196ee9c:14794:16156 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->8
5a347196ee9c:14794:16156 [0] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14798:16158 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
5a347196ee9c:14796:16162 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
5a347196ee9c:14797:16160 [3] NCCL INFO comm 0x5651b78afc80 rank 3 nRanks 16 nNodes 2 localRanks 8 localRank 3 MNNVL 0
5a347196ee9c:14798:16158 [4] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14796:16162 [2] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14797:16160 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
5a347196ee9c:14797:16160 [3] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14800:16161 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14799:16163 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14800:16161 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14799:16163 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14797:16160 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14798:16158 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14796:16162 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14797:16160 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14798:16158 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14796:16162 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/0 : 9[1] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 01/0 : 1[1] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14801:16157 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14801:16157 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14798:16158 [4] NCCL INFO Connected all rings
5a347196ee9c:14795:16159 [1] NCCL INFO Connected all rings
5a347196ee9c:14797:16160 [3] NCCL INFO Connected all rings
5a347196ee9c:14796:16162 [2] NCCL INFO Connected all rings
5a347196ee9c:14794:16156 [0] NCCL INFO Connected all rings
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14797:16160 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14798:16158 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14796:16162 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14797:16160 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14798:16158 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14796:16162 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14794:16156 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14795:16159 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
5a347196ee9c:14799:16163 [5] NCCL INFO Connected all rings
5a347196ee9c:14801:16157 [7] NCCL INFO Connected all rings
5a347196ee9c:14800:16161 [6] NCCL INFO Connected all rings
5a347196ee9c:14800:16161 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14799:16163 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14800:16161 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14799:16163 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14801:16157 [7] NCCL INFO Connected all trees
5a347196ee9c:14801:16157 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14801:16157 [7] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14797:16160 [3] NCCL INFO Connected all trees
5a347196ee9c:14797:16160 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14797:16160 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14800:16161 [6] NCCL INFO Connected all trees
5a347196ee9c:14800:16161 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14800:16161 [6] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14798:16158 [4] NCCL INFO Connected all trees
5a347196ee9c:14799:16163 [5] NCCL INFO Connected all trees
5a347196ee9c:14798:16158 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14799:16163 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14798:16158 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14799:16163 [5] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14796:16162 [2] NCCL INFO Connected all trees
5a347196ee9c:14796:16162 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14796:16162 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14795:16159 [1] NCCL INFO Connected all trees
5a347196ee9c:14795:16159 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14795:16159 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14794:16156 [0] NCCL INFO Connected all trees
5a347196ee9c:14794:16156 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14794:16156 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14799:16163 [5] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14794:16156 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14799:16163 [5] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14794:16156 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14799:16163 [5] NCCL INFO ncclCommInitRank comm 0x55c5eea584c0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14794:16156 [0] NCCL INFO ncclCommInitRank comm 0x561e645335d0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14800:16161 [6] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14800:16161 [6] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14800:16161 [6] NCCL INFO ncclCommInitRank comm 0x55af1a184950 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14801:16157 [7] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14801:16157 [7] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14801:16157 [7] NCCL INFO ncclCommInitRank comm 0x562054163700 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14796:16162 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14796:16162 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14795:16159 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14796:16162 [2] NCCL INFO ncclCommInitRank comm 0x55ea0e49f520 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14795:16159 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14795:16159 [1] NCCL INFO ncclCommInitRank comm 0x55f175e018b0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14797:16160 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14797:16160 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14797:16160 [3] NCCL INFO ncclCommInitRank comm 0x5651b78afc80 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xa0364583bc190008 - Init COMPLETE
5a347196ee9c:14798:16158 [4] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
5a347196ee9c:14798:16158 [4] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
5a347196ee9c:14798:16158 [4] NCCL INFO ncclCommInitRank comm 0x563c393acfe0 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xa0364583bc190008 - Init COMPLETE
Running tokenizer on dataset (num_proc=16):   0% 0/1090 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6% 69/1090 [00:00<00:06, 152.35 examples/s]Running tokenizer on dataset (num_proc=16):  13% 138/1090 [00:00<00:03, 249.94 examples/s]Running tokenizer on dataset (num_proc=16):  31% 342/1090 [00:00<00:01, 519.83 examples/s]Running tokenizer on dataset (num_proc=16):  44% 478/1090 [00:00<00:00, 627.00 examples/s]Running tokenizer on dataset (num_proc=16):  56% 614/1090 [00:01<00:00, 702.69 examples/s]Running tokenizer on dataset (num_proc=16):  75% 818/1090 [00:01<00:00, 883.20 examples/s]Running tokenizer on dataset (num_proc=16):  88% 954/1090 [00:01<00:00, 919.02 examples/s]Running tokenizer on dataset (num_proc=16): 100% 1090/1090 [00:01<00:00, 914.19 examples/s]Running tokenizer on dataset (num_proc=16): 100% 1090/1090 [00:01<00:00, 662.87 examples/s]
training example:
input_ids:
[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91, 408, 8757, 842, 91, 1339, 6023, 151665, 27, 91, 2468, 8757, 842, 91, 29, 77091, 27, 91, 408, 8757, 842, 91, 1339, 9707, 0, 358, 1079, 5867, 606, 38154, 458, 15235, 17847, 7881, 553, 5867, 3094, 3417, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151665]
inputs:
<|start_header_id|>user<|end_header_id|>

hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9707, 0, 358, 1079, 5867, 606, 38154, 458, 15235, 17847, 7881, 553, 5867, 3094, 3417, 13, 2585, 646, 358, 7789, 498, 3351, 30, 151665]
labels:
Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<|eot_id|>
[INFO|configuration_utils.py:750] 2025-08-15 16:52:13,617 >> loading configuration file /data/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:817] 2025-08-15 16:52:13,618 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-08-15 16:52:13] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1305] 2025-08-15 16:52:13,721 >> loading weights file /data/Qwen2.5-32B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:4363] 2025-08-15 16:52:13,721 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-08-15 16:52:13,721] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[INFO|configuration_utils.py:1098] 2025-08-15 16:52:13,734 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[2025-08-15 16:52:13,834] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,840] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,840] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,843] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,848] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,852] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:52:13,855] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:53:11,832] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0% 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.32s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.34s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.36s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.35s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.32s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.32s/it]Loading checkpoint shards:   6% 1/17 [00:08<02:13,  8.33s/it]Loading checkpoint shards:   6% 1/17 [00:09<02:25,  9.07s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.90s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.91s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.90s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.90s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.92s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.92s/it]Loading checkpoint shards:  12% 2/17 [00:15<01:58,  7.91s/it]Loading checkpoint shards:  12% 2/17 [00:16<02:03,  8.21s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.71s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:23<01:47,  7.70s/it]Loading checkpoint shards:  18% 3/17 [00:24<01:50,  7.87s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:30<01:39,  7.63s/it]Loading checkpoint shards:  24% 4/17 [00:31<01:40,  7.73s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:38<01:31,  7.59s/it]Loading checkpoint shards:  29% 5/17 [00:39<01:31,  7.65s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:45<01:23,  7.56s/it]Loading checkpoint shards:  35% 6/17 [00:46<01:23,  7.60s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.54s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.54s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.55s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.55s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.55s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.54s/it]Loading checkpoint shards:  41% 7/17 [00:53<01:15,  7.54s/it]Loading checkpoint shards:  41% 7/17 [00:54<01:15,  7.57s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:00<01:07,  7.53s/it]Loading checkpoint shards:  47% 8/17 [01:01<01:07,  7.54s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:08<01:00,  7.52s/it]Loading checkpoint shards:  53% 9/17 [01:09<01:00,  7.53s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:15<00:52,  7.50s/it]Loading checkpoint shards:  59% 10/17 [01:16<00:52,  7.51s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:23<00:45,  7.50s/it]Loading checkpoint shards:  65% 11/17 [01:24<00:45,  7.51s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:30<00:37,  7.50s/it]Loading checkpoint shards:  71% 12/17 [01:31<00:37,  7.50s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:38<00:29,  7.49s/it]Loading checkpoint shards:  76% 13/17 [01:39<00:29,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:45<00:22,  7.49s/it]Loading checkpoint shards:  82% 14/17 [01:46<00:22,  7.49s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:53<00:15,  7.50s/it]Loading checkpoint shards:  88% 15/17 [01:54<00:15,  7.50s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:00<00:07,  7.51s/it]Loading checkpoint shards:  94% 16/17 [02:01<00:07,  7.51s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.22s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.22s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.23s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.23s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.22s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.23s/it]
Loading checkpoint shards: 100% 17/17 [02:02<00:00,  5.83s/it]Loading checkpoint shards: 100% 17/17 [02:02<00:00,  7.22s/it]
Loading checkpoint shards: 100% 17/17 [02:06<00:00,  6.62s/it]Loading checkpoint shards: 100% 17/17 [02:06<00:00,  7.42s/it]
[INFO|modeling_utils.py:5606] 2025-08-15 16:55:18,064 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:5614] 2025-08-15 16:55:18,064 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data/Qwen2.5-32B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-08-15 16:55:18,068 >> loading configuration file /data/Qwen2.5-32B-Instruct/generation_config.json
[INFO|configuration_utils.py:1098] 2025-08-15 16:55:18,069 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-08-15 16:55:18] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-15 16:55:18] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-15 16:55:18] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-08-15 16:55:18] llamafactory.model.adapter:143 >> Fine-tuning method: Full
[INFO|2025-08-15 16:55:18] llamafactory.model.loader:143 >> trainable params: 32,763,876,352 || all params: 32,763,876,352 || trainable%: 100.0000
[INFO|trainer.py:757] 2025-08-15 16:55:18,092 >> Using auto half precision backend
[2025-08-15 16:55:19,474] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-08-15 16:55:19,474] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 16
[2025-08-15 16:55:19,486] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-08-15 16:55:19,487] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-08-15 16:55:19,487] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-15 16:55:19,525] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-08-15 16:55:19,525] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-08-15 16:55:19,525] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-08-15 16:55:19,525] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-08-15 16:55:19,701] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-08-15 16:55:19,702] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 8.07 GB         CA 3.83 GB         Max_CA 8 GB 
[2025-08-15 16:55:19,702] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.9 GB, percent = 2.4%
[2025-08-15 16:55:19,704] [INFO] [stage3.py:170:__init__] Reduce bucket size 26214400
[2025-08-15 16:55:19,704] [INFO] [stage3.py:171:__init__] Prefetch bucket size 23592960
[2025-08-15 16:55:19,868] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-08-15 16:55:19,869] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 3.81 GB         CA 3.83 GB         Max_CA 4 GB 
[2025-08-15 16:55:19,869] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.9 GB, percent = 2.4%
Parameter Offload: Total persistent parameters: 1119232 in 321 params
[2025-08-15 16:55:20,122] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-08-15 16:55:20,122] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 3.81 GB         CA 3.83 GB         Max_CA 4 GB 
[2025-08-15 16:55:20,122] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.91 GB, percent = 2.4%
[2025-08-15 16:55:20,301] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-08-15 16:55:20,301] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 3.81 GB         CA 3.83 GB         Max_CA 4 GB 
[2025-08-15 16:55:20,301] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.91 GB, percent = 2.4%
[2025-08-15 16:55:23,415] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 3
[2025-08-15 16:55:23,416] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 3.81 GB         CA 3.83 GB         Max_CA 4 GB 
[2025-08-15 16:55:23,416] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.23 GB, percent = 2.4%
[2025-08-15 16:55:23,601] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-08-15 16:55:23,601] [INFO] [utils.py:782:see_memory_usage] MA 3.81 GB         Max_MA 3.81 GB         CA 3.83 GB         Max_CA 4 GB 
[2025-08-15 16:55:23,602] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 23.99 GB, percent = 2.4%
[2025-08-15 16:55:23,818] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-08-15 16:55:23,819] [INFO] [utils.py:782:see_memory_usage] MA 11.44 GB         Max_MA 13.38 GB         CA 15.28 GB         Max_CA 15 GB 
[2025-08-15 16:55:23,819] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.0 GB, percent = 2.4%
[2025-08-15 16:55:24,001] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-15 16:55:24,002] [INFO] [utils.py:782:see_memory_usage] MA 11.44 GB         Max_MA 11.44 GB         CA 15.28 GB         Max_CA 15 GB 
[2025-08-15 16:55:24,002] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.04 GB, percent = 2.4%
[2025-08-15 16:55:24,195] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-15 16:55:24,195] [INFO] [utils.py:782:see_memory_usage] MA 11.44 GB         Max_MA 15.32 GB         CA 19.14 GB         Max_CA 19 GB 
[2025-08-15 16:55:24,195] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.04 GB, percent = 2.4%
[2025-08-15 16:55:24,196] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-08-15 16:56:20,632] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-15 16:56:20,633] [INFO] [utils.py:782:see_memory_usage] MA 15.31 GB         Max_MA 18.21 GB         CA 19.14 GB         Max_CA 19 GB 
[2025-08-15 16:56:20,633] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.17 GB, percent = 2.5%
[2025-08-15 16:56:20,633] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-08-15 16:56:20,633] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-08-15 16:56:20,633] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-08-15 16:56:20,633] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-08-15 16:56:20,635] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-08-15 16:56:20,635] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-15 16:56:20,635] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-15 16:56:20,635] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-08-15 16:56:20,635] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-08-15 16:56:20,635] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1eab778b50>
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-08-15 16:56:20,636] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   train_batch_size ............. 128
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   world_size ................... 16
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-15 16:56:20,637] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-08-15 16:56:20,637] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2433] 2025-08-15 16:56:20,638 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-15 16:56:20,638 >>   Num examples = 1,090
[INFO|trainer.py:2435] 2025-08-15 16:56:20,638 >>   Num Epochs = 3
[INFO|trainer.py:2436] 2025-08-15 16:56:20,638 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2439] 2025-08-15 16:56:20,638 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2440] 2025-08-15 16:56:20,638 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-15 16:56:20,638 >>   Total optimization steps = 27
[INFO|trainer.py:2442] 2025-08-15 16:56:20,640 >>   Number of trainable parameters = 32,763,876,352
  0% 0/27 [00:00<?, ?it/s]5a347196ee9c:14794:16513 [0] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14801:16504 [7] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14798:16485 [4] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14796:16475 [2] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14800:16511 [6] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14799:16470 [5] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14794:16522 [0] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14794:16522 [0] NCCL INFO Using network Socket
5a347196ee9c:14795:16458 [1] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14801:16523 [7] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14801:16523 [7] NCCL INFO Using network Socket
5a347196ee9c:14798:16524 [4] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14798:16524 [4] NCCL INFO Using network Socket
5a347196ee9c:14796:16525 [2] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14796:16525 [2] NCCL INFO Using network Socket
5a347196ee9c:14799:16527 [5] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14800:16526 [6] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14799:16527 [5] NCCL INFO Using network Socket
5a347196ee9c:14800:16526 [6] NCCL INFO Using network Socket
5a347196ee9c:14795:16528 [1] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14795:16528 [1] NCCL INFO Using network Socket
5a347196ee9c:14797:16492 [3] NCCL INFO Comm config Blocking set to 1
5a347196ee9c:14797:16529 [3] NCCL INFO Using non-device net plugin version 0
5a347196ee9c:14797:16529 [3] NCCL INFO Using network Socket
5a347196ee9c:14800:16526 [6] NCCL INFO ncclCommInitRank comm 0x7f64fc069750 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14801:16523 [7] NCCL INFO ncclCommInitRank comm 0x7fc40806e4d0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14799:16527 [5] NCCL INFO ncclCommInitRank comm 0x7f623406e4f0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14798:16524 [4] NCCL INFO ncclCommInitRank comm 0x7f9668069650 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14797:16529 [3] NCCL INFO ncclCommInitRank comm 0x7f422c0d6a90 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14796:16525 [2] NCCL INFO ncclCommInitRank comm 0x7f353006d6f0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14795:16528 [1] NCCL INFO ncclCommInitRank comm 0x7fe29c06e690 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14794:16522 [0] NCCL INFO ncclCommInitRank comm 0x7f1e08069f40 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xd019db1c768eabe6 - Init START
5a347196ee9c:14801:16523 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000
5a347196ee9c:14801:16523 [7] NCCL INFO NVLS multicast support is not available on dev 7
5a347196ee9c:14799:16527 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000
5a347196ee9c:14799:16527 [5] NCCL INFO NVLS multicast support is not available on dev 5
5a347196ee9c:14800:16526 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000
5a347196ee9c:14800:16526 [6] NCCL INFO NVLS multicast support is not available on dev 6
5a347196ee9c:14798:16524 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000
5a347196ee9c:14798:16524 [4] NCCL INFO NVLS multicast support is not available on dev 4
5a347196ee9c:14797:16529 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
5a347196ee9c:14797:16529 [3] NCCL INFO NVLS multicast support is not available on dev 3
5a347196ee9c:14795:16528 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff
5a347196ee9c:14794:16522 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
5a347196ee9c:14794:16522 [0] NCCL INFO NVLS multicast support is not available on dev 0
5a347196ee9c:14796:16525 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
5a347196ee9c:14795:16528 [1] NCCL INFO NVLS multicast support is not available on dev 1
5a347196ee9c:14796:16525 [2] NCCL INFO NVLS multicast support is not available on dev 2
5a347196ee9c:14800:16526 [6] NCCL INFO comm 0x7f64fc069750 rank 6 nRanks 16 nNodes 2 localRanks 8 localRank 6 MNNVL 0
5a347196ee9c:14799:16527 [5] NCCL INFO comm 0x7f623406e4f0 rank 5 nRanks 16 nNodes 2 localRanks 8 localRank 5 MNNVL 0
5a347196ee9c:14801:16523 [7] NCCL INFO comm 0x7fc40806e4d0 rank 7 nRanks 16 nNodes 2 localRanks 8 localRank 7 MNNVL 0
5a347196ee9c:14797:16529 [3] NCCL INFO comm 0x7f422c0d6a90 rank 3 nRanks 16 nNodes 2 localRanks 8 localRank 3 MNNVL 0
5a347196ee9c:14799:16527 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
5a347196ee9c:14800:16526 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
5a347196ee9c:14799:16527 [5] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14800:16526 [6] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14801:16523 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
5a347196ee9c:14801:16523 [7] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14797:16529 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
5a347196ee9c:14798:16524 [4] NCCL INFO comm 0x7f9668069650 rank 4 nRanks 16 nNodes 2 localRanks 8 localRank 4 MNNVL 0
5a347196ee9c:14797:16529 [3] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14796:16525 [2] NCCL INFO comm 0x7f353006d6f0 rank 2 nRanks 16 nNodes 2 localRanks 8 localRank 2 MNNVL 0
5a347196ee9c:14795:16528 [1] NCCL INFO comm 0x7fe29c06e690 rank 1 nRanks 16 nNodes 2 localRanks 8 localRank 1 MNNVL 0
5a347196ee9c:14794:16522 [0] NCCL INFO comm 0x7f1e08069f40 rank 0 nRanks 16 nNodes 2 localRanks 8 localRank 0 MNNVL 0
5a347196ee9c:14798:16524 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
5a347196ee9c:14798:16524 [4] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14796:16525 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
5a347196ee9c:14795:16528 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/02 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
5a347196ee9c:14796:16525 [2] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14795:16528 [1] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/02 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
5a347196ee9c:14794:16522 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->8
5a347196ee9c:14794:16522 [0] NCCL INFO P2P Chunksize set to 131072
5a347196ee9c:14799:16527 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14800:16526 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14796:16525 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14797:16529 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14799:16527 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14798:16524 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14800:16526 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14796:16525 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14797:16529 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14798:16524 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/0 : 9[1] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/0 : 9[1] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/0 : 0[0] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 00/0 : 1[1] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 01/0 : 1[1] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14801:16523 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14801:16523 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14798:16524 [4] NCCL INFO Connected all rings
5a347196ee9c:14796:16525 [2] NCCL INFO Connected all rings
5a347196ee9c:14797:16529 [3] NCCL INFO Connected all rings
5a347196ee9c:14795:16528 [1] NCCL INFO Connected all rings
5a347196ee9c:14798:16524 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14798:16524 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
5a347196ee9c:14796:16525 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14796:16525 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
5a347196ee9c:14797:16529 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
5a347196ee9c:14797:16529 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
5a347196ee9c:14794:16522 [0] NCCL INFO Connected all rings
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14794:16522 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/Socket/0
5a347196ee9c:14795:16528 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
5a347196ee9c:14801:16523 [7] NCCL INFO Connected all rings
5a347196ee9c:14800:16526 [6] NCCL INFO Connected all rings
5a347196ee9c:14799:16527 [5] NCCL INFO Connected all rings
5a347196ee9c:14799:16527 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14800:16526 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14799:16527 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
5a347196ee9c:14800:16526 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
5a347196ee9c:14798:16524 [4] NCCL INFO Connected all trees
5a347196ee9c:14797:16529 [3] NCCL INFO Connected all trees
5a347196ee9c:14798:16524 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14798:16524 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14797:16529 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14797:16529 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14801:16523 [7] NCCL INFO Connected all trees
5a347196ee9c:14801:16523 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14801:16523 [7] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14799:16527 [5] NCCL INFO Connected all trees
5a347196ee9c:14800:16526 [6] NCCL INFO Connected all trees
5a347196ee9c:14799:16527 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14799:16527 [5] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14800:16526 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14800:16526 [6] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14796:16525 [2] NCCL INFO Connected all trees
5a347196ee9c:14796:16525 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14796:16525 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14795:16528 [1] NCCL INFO Connected all trees
5a347196ee9c:14794:16522 [0] NCCL INFO Connected all trees
5a347196ee9c:14795:16528 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14795:16528 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14794:16522 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
5a347196ee9c:14794:16522 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer
5a347196ee9c:14798:16524 [4] NCCL INFO ncclCommInitRank comm 0x7f9668069650 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId b5000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14801:16523 [7] NCCL INFO ncclCommInitRank comm 0x7fc40806e4d0 rank 7 nranks 16 cudaDev 7 nvmlDev 7 busId cf000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14797:16529 [3] NCCL INFO ncclCommInitRank comm 0x7f422c0d6a90 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId 20000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14794:16522 [0] NCCL INFO ncclCommInitRank comm 0x7f1e08069f40 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId e000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14799:16527 [5] NCCL INFO ncclCommInitRank comm 0x7f623406e4f0 rank 5 nranks 16 cudaDev 5 nvmlDev 5 busId b6000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14795:16528 [1] NCCL INFO ncclCommInitRank comm 0x7fe29c06e690 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId f000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14796:16525 [2] NCCL INFO ncclCommInitRank comm 0x7f353006d6f0 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 1f000 commId 0xd019db1c768eabe6 - Init COMPLETE
5a347196ee9c:14800:16526 [6] NCCL INFO ncclCommInitRank comm 0x7f64fc069750 rank 6 nranks 16 cudaDev 6 nvmlDev 6 busId ce000 commId 0xd019db1c768eabe6 - Init COMPLETE
^[[B^[[B^[[B^[[B                ^[[B^[[B[2025-08-15 17:19:06,303] [WARNING] [stage3.py:2148:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  4% 1/27 [22:45<9:51:47, 1365.67s/it]  7% 2/27 [44:30<9:14:11, 1330.06s/it] 11% 3/27 [1:06:17<8:47:41, 1319.22s/it] 15% 4/27 [1:28:03<8:23:44, 1314.12s/it] 19% 5/27 [1:49:49<8:00:48, 1311.30s/it] 22% 6/27 [2:11:35<7:38:19, 1309.48s/it] 26% 7/27 [2:33:21<7:16:05, 1308.26s/it] 30% 8/27 [2:55:07<6:54:06, 1307.70s/it] 33% 9/27 [3:08:44<5:46:15, 1154.22s/it] 37% 10/27 [3:30:30<5:40:15, 1200.93s/it]                                         {'loss': 1.0324, 'grad_norm': 2.869866149965595, 'learning_rate': 8.535533905932738e-05, 'epoch': 1.12}
 37% 10/27 [3:30:30<5:40:15, 1200.93s/it]^[[A^[[A 41% 11/27 [3:52:14<5:28:42, 1232.63s/it] 44% 12/27 [4:14:03<5:13:58, 1255.89s/it] 48% 13/27 [4:35:52<4:56:47, 1271.99s/it] 52% 14/27 [4:57:40<4:37:54, 1282.65s/it] 56% 15/27 [5:19:27<4:18:01, 1290.17s/it] 59% 16/27 [5:41:14<3:57:26, 1295.16s/it] 63% 17/27 [6:03:00<3:36:25, 1298.53s/it] 67% 18/27 [6:16:38<2:53:06, 1154.04s/it] 70% 19/27 [6:38:26<2:40:01, 1200.21s/it] 74% 20/27 [7:00:11<2:23:41, 1231.61s/it]                                         {'loss': 0.2811, 'grad_norm': 0.5076579966089604, 'learning_rate': 2.500000000000001e-05, 'epoch': 2.23}
 74% 20/27 [7:00:11<2:23:41, 1231.61s/it] 78% 21/27 [7:21:58<2:05:26, 1254.45s/it] 81% 22/27 [7:43:44<1:45:49, 1269.87s/it] 85% 23/27 [8:05:27<1:25:19, 1279.91s/it] 89% 24/27 [8:27:13<1:04:22, 1287.65s/it] 93% 25/27 [8:48:58<43:05, 1292.76s/it]   96% 26/27 [9:10:43<21:36, 1296.43s/it]100% 27/27 [9:24:17<00:00, 1151.83s/it][INFO|trainer.py:4074] 2025-08-16 02:23:05,556 >> Saving model checkpoint to /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27
[INFO|configuration_utils.py:478] 2025-08-16 02:23:05,560 >> Configuration saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/config.json
[INFO|configuration_utils.py:869] 2025-08-16 02:23:05,561 >> Configuration saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/generation_config.json
[INFO|modeling_utils.py:4180] 2025-08-16 02:24:04,089 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 14 checkpoint shards. You can find where each parameters has been saved in the index located at /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2393] 2025-08-16 02:24:04,090 >> chat template saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-16 02:24:04,091 >> tokenizer config file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-16 02:24:04,091 >> Special tokens file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/special_tokens_map.json
[2025-08-16 02:24:04,245] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step25 is about to be saved!
[2025-08-16 02:24:04,257] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-08-16 02:24:04,257] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-08-16 02:24:05,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-08-16 02:24:05,848] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-08-16 02:24:44,082] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-08-16 02:24:44,082] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/checkpoint-27/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-08-16 02:27:19,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25 is ready now!
[INFO|trainer.py:2718] 2025-08-16 02:27:19,248 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                       {'train_runtime': 34258.608, 'train_samples_per_second': 0.095, 'train_steps_per_second': 0.001, 'train_loss': 0.5031548650176437, 'epoch': 3.0}
100% 27/27 [9:30:58<00:00, 1151.83s/it]100% 27/27 [9:30:58<00:00, 1268.84s/it]
[INFO|trainer.py:4074] 2025-08-16 02:29:45,328 >> Saving model checkpoint to /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft
[INFO|configuration_utils.py:478] 2025-08-16 02:29:45,333 >> Configuration saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/config.json
[INFO|configuration_utils.py:869] 2025-08-16 02:29:45,333 >> Configuration saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/generation_config.json

5a347196ee9c:14795:16855 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14795:16855 [0] NCCL INFO group.cc:64 -> 1 [Async thread]

5a347196ee9c:14798:16852 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14798:16852 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14795:14795 [1] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14795:14795 [1] NCCL INFO init.cc:2038 -> 1
5a347196ee9c:14798:14798 [4] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14798:14798 [4] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:45] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.
[WARNING|2025-08-16 02:29:45] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.

5a347196ee9c:14801:16851 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14801:16851 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14801:14801 [7] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14801:14801 [7] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:45] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.

5a347196ee9c:14796:16857 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14796:16857 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14796:14796 [2] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14796:14796 [2] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:46] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.

5a347196ee9c:14797:16856 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14797:16856 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14797:14797 [3] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14797:14797 [3] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:46] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.

5a347196ee9c:14800:16854 [0] init.cc:1962 NCCL WARN Cuda failure 'CUDA-capable device(s) is/are busy or unavailable'
5a347196ee9c:14800:16854 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14800:14800 [6] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14800:14800 [6] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:46] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'CUDA-capable device(s) is/are busy or unavailable'.

5a347196ee9c:14799:16853 [0] init.cc:1962 NCCL WARN Cuda failure 'out of memory'
5a347196ee9c:14799:16853 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
5a347196ee9c:14799:14799 [5] NCCL INFO group.cc:418 -> 1
5a347196ee9c:14799:14799 [5] NCCL INFO init.cc:2038 -> 1
[WARNING|2025-08-16 02:29:46] llamafactory.train.tuner:91 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.
[INFO|modeling_utils.py:4180] 2025-08-16 02:30:44,696 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 14 checkpoint shards. You can find where each parameters has been saved in the index located at /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2393] 2025-08-16 02:30:44,697 >> chat template saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-16 02:30:44,697 >> tokenizer config file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-16 02:30:44,698 >> Special tokens file saved in /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  total_flos               =     3873GF
  train_loss               =     0.5032
  train_runtime            = 9:30:58.60
  train_samples_per_second =      0.095
  train_steps_per_second   =      0.001
Figure saved at: /data/llamafactory/saves/qwen2_5vl-32b/lora/full_sft/training_loss.png
[WARNING|2025-08-16 02:30:46] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-08-16 02:30:46] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:456] 2025-08-16 02:30:46,459 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
5a347196ee9c:14794:14794 [0] NCCL INFO comm 0x7f1e08069f40 rank 0 nranks 16 cudaDev 0 busId e000 - Destroy COMPLETE
5a347196ee9c:14794:14794 [0] NCCL INFO comm 0x561e645335d0 rank 0 nranks 16 cudaDev 0 busId e000 - Destroy COMPLETE
